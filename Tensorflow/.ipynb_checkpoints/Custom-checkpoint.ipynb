{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "duplicate-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "administrative-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-night",
   "metadata": {},
   "source": [
    "## Lambda Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.abs(x)), # you can use lambda here\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "         loss=\"sparse_categorical_crossentropy\",\n",
    "         metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5)\n",
    "print(model.metrics_names)\n",
    "print(f\"lambda{i}: {model.evaluate(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-sterling",
   "metadata": {},
   "source": [
    "## Custom Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "operating-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custome relu\n",
    "def my_relu(x):\n",
    "    return K.maximum(-0.5, x) # Arbitrary number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "innocent-duncan",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4217 - accuracy: 0.8764\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1242 - accuracy: 0.9636\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0809 - accuracy: 0.9756\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0561 - accuracy: 0.9831\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0453 - accuracy: 0.9861\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0762 - accuracy: 0.9766\n",
      "lambda-0.8: [0.0762462392449379, 0.9765999913215637]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4272 - accuracy: 0.8787\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1291 - accuracy: 0.9613\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0837 - accuracy: 0.9751\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0621 - accuracy: 0.9812\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0442 - accuracy: 0.9863\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0847 - accuracy: 0.9749\n",
      "lambda-0.6222222222222222: [0.0847073346376419, 0.9749000072479248]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4172 - accuracy: 0.8783\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1262 - accuracy: 0.9636\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0818 - accuracy: 0.9761\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0534 - accuracy: 0.9839\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0411 - accuracy: 0.9878\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0710 - accuracy: 0.9776\n",
      "lambda-0.4444444444444445: [0.0709649920463562, 0.9775999784469604]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4231 - accuracy: 0.8743\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1228 - accuracy: 0.9645\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0828 - accuracy: 0.9759\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0584 - accuracy: 0.9822\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0435 - accuracy: 0.9865\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0762 - accuracy: 0.9765\n",
      "lambda-0.2666666666666667: [0.07622218877077103, 0.9764999747276306]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4165 - accuracy: 0.8792\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1290 - accuracy: 0.9617\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0835 - accuracy: 0.9748\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0595 - accuracy: 0.9815\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0424 - accuracy: 0.9868\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0746 - accuracy: 0.9780\n",
      "lambda-0.0888888888888889: [0.07464494556188583, 0.9779999852180481]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4317 - accuracy: 0.8753\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1257 - accuracy: 0.9647\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0799 - accuracy: 0.9768\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0598 - accuracy: 0.9823\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0449 - accuracy: 0.9861\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0779 - accuracy: 0.9768: 0s - loss: 0.1092 - \n",
      "lambda0.0888888888888889: [0.0778607577085495, 0.9768000245094299]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4334 - accuracy: 0.8730\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1295 - accuracy: 0.9620\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0799 - accuracy: 0.9748\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0563 - accuracy: 0.9831\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0423 - accuracy: 0.9872\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0707 - accuracy: 0.9779\n",
      "lambda0.2666666666666666: [0.07073909044265747, 0.9779000282287598]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4289 - accuracy: 0.8755\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1314 - accuracy: 0.9617\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0804 - accuracy: 0.9762\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0587 - accuracy: 0.9823\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0443 - accuracy: 0.9865\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0699 - accuracy: 0.9782\n",
      "lambda0.4444444444444444: [0.06988456100225449, 0.9782000184059143]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4139 - accuracy: 0.8823\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1326 - accuracy: 0.9609\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0832 - accuracy: 0.9755\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0579 - accuracy: 0.9827\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0434 - accuracy: 0.9863\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0721 - accuracy: 0.9768\n",
      "lambda0.6222222222222222: [0.0720597505569458, 0.9768000245094299]\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4218 - accuracy: 0.8783\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1242 - accuracy: 0.9627\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0796 - accuracy: 0.9758\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0561 - accuracy: 0.9831\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0404 - accuracy: 0.9885\n",
      "['loss', 'accuracy']\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.0732 - accuracy: 0.9763\n",
      "lambda0.8: [0.07319613546133041, 0.9763000011444092]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in np.linspace(-0.8, 0.8, 10):\n",
    "    def my_relu(x):\n",
    "        return K.maximum(-0.5, x)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128),\n",
    "        tf.keras.layers.Lambda(my_relu), # Replace by custom one\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=\"adam\",\n",
    "             loss=\"sparse_categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=5)\n",
    "    print(model.metrics_names)\n",
    "    print(f\"lambda{i}: {model.evaluate(X_test, y_test)}\")\n",
    "    model.evaluate(X_test, y_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-contribution",
   "metadata": {},
   "source": [
    "|Loss,| Accuracy| Model |\n",
    "|-----|---------|-------|\n",
    "|0.10067794471979141| 0.9725000262260437 |with lambda layer |\n",
    "|0.28765377402305603| 0.9217000007629395|without lambda layer | \n",
    "|0.07813046127557755| 0.9767000079154968 |with 0.4 my relu |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-coating",
   "metadata": {},
   "source": [
    "## Custom Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "angry-toddler",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "leading-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attended-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(Layer):\n",
    "    \n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape): # Create the state of the layer(weights)\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(name=\"kernel\",\n",
    "                initial_value = w_init(shape=(input_shape[-1], self.units), dtype=\"float32\"),\n",
    "                trainable=True)\n",
    "        \n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(name=\"bias\",\n",
    "                initial_value=b_init(shape=(self.units), dtype=\"float32\"),\n",
    "                trainable=True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs): # Defines the computation from inputs to outputs\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-decline",
   "metadata": {},
   "source": [
    "## With activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "israeli-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDense(Layer):\n",
    "\n",
    "    # add an activation parameter\n",
    "    def __init__(self, units=32, activation=None):\n",
    "        super(SimpleDense, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        # define the activation to get from the built-in activation layers in Keras\n",
    "        self.activation = tf.keras.activations.get(activation)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(name=\"kernel\",\n",
    "            initial_value=w_init(shape=(input_shape[-1], self.units),\n",
    "                                 dtype='float32'),    \n",
    "            trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(name=\"bias\",\n",
    "            initial_value=b_init(shape=(self.units,), dtype='float32'),\n",
    "            trainable=True)\n",
    "        #super().build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # pass the computation to the activation layer\n",
    "        return self.activation(tf.matmul(inputs, self.w) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "elder-desktop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x250adbf76a0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# y = 2x -1\n",
    "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
    "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
    "\n",
    "model = tf.keras.Sequential([SimpleDense(units=1)])\n",
    "model.compile(optimizer=\"sgd\",loss=\"mean_squared_error\")\n",
    "model.fit(x, y, epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "other-cancer",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/kernel:0' shape=(1, 1) dtype=float32>\n",
      "<tf.Variable 'simple_dense_14/bias:0' shape=(1,) dtype=float32>\n",
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "exact-programming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.04403572207069838\n",
      "0.17614288828279354\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Cost function\n",
    "def nn(w,x,b):\n",
    "    return w*(x**2) + b\n",
    "\n",
    "w = random.random()\n",
    "b = random.random()\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "x = 2\n",
    "y = 0\n",
    "\n",
    "def update_params():\n",
    "    global w, b\n",
    "    y_pred = nn(w,x,b)\n",
    "    #print(y_pred)\n",
    "    derivative_wrt_w = -2*x**2*(y - y_pred) # partial derivative\n",
    "    derivative_wrt_b = -2*(y - y_pred) # partial derivative\n",
    "    w = w - lr*derivative_wrt_w # Update w\n",
    "    b = b - lr*derivative_wrt_b # Update b\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    update_params()\n",
    "\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-canon",
   "metadata": {},
   "source": [
    "## Custom Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_begin(self, batch, log=None):\n",
    "        print(\"Training: batch {} begins at {}\".\n",
    "                     format(batch, datetime.datetime.now().time()))\n",
    "        \n",
    "    def on_train_batch_end(self, batch, log=None):\n",
    "        print(\"Training: batch {} ends at {}\".\n",
    "                     format(batch, datetime.datetime.now().time()))\n",
    "        \n",
    "\n",
    "my_custom_callback = MyCustomCallback()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-spray",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Randomly sample data\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
