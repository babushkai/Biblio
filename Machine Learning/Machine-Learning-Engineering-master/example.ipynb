{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7280b53-a88e-45c2-8042-11449fc1b6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing label.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile label.py\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class LabelEncoder:\n",
    "    \"\"\"Encode labels into unique indices.\n",
    "    ```python\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(labels)\n",
    "    y = label_encoder.encode(labels)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_to_index: Dict = {}) -> None:\n",
    "        \"\"\"Initialize the label encoder.\n",
    "        Args:\n",
    "            class_to_index (Dict, optional): mapping between classes and unique indices. Defaults to {}.\n",
    "        \"\"\"\n",
    "        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_to_index)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
    "\n",
    "    def fit(self, y: List):\n",
    "        \"\"\"Fit a list of labels to the encoder.\n",
    "        Args:\n",
    "            y (List): raw labels.\n",
    "        Returns:\n",
    "            Fitted LabelEncoder instance.\n",
    "        \"\"\"\n",
    "        classes = np.unique(y)\n",
    "        for i, class_ in enumerate(classes):\n",
    "            self.class_to_index[class_] = i\n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        self.classes = list(self.class_to_index.keys())\n",
    "        return self\n",
    "\n",
    "    def encode(self, y: List) -> np.ndarray:\n",
    "        \"\"\"Encode a list of raw labels.\n",
    "        Args:\n",
    "            y (List): raw labels.\n",
    "        Returns:\n",
    "            np.ndarray: encoded labels as indices.\n",
    "        \"\"\"\n",
    "        encoded = np.zeros((len(y)), dtype=int)\n",
    "        for i, item in enumerate(y):\n",
    "            encoded[i] = self.class_to_index[item]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, y: List) -> List:\n",
    "        \"\"\"Decode a list of indices.\n",
    "        Args:\n",
    "            y (List): indices.\n",
    "        Returns:\n",
    "            List: labels.\n",
    "        \"\"\"\n",
    "        classes = []\n",
    "        for i, item in enumerate(y):\n",
    "            classes.append(self.index_to_class[item])\n",
    "        return classes\n",
    "\n",
    "    def save(self, fp: str) -> None:\n",
    "        \"\"\"Save class instance to JSON file.\n",
    "        Args:\n",
    "            fp (str): filepath to save to.\n",
    "        \"\"\"\n",
    "        with open(fp, \"w\") as fp:\n",
    "            contents = {\"class_to_index\": self.class_to_index}\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fp: str):\n",
    "        \"\"\"Load instance of LabelEncoder from file.\n",
    "        Args:\n",
    "            fp (str): JSON filepath to load from.\n",
    "        Returns:\n",
    "            LabelEncoder instance.\n",
    "        \"\"\"\n",
    "        with open(fp) as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        return cls(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e705d2a4-63bf-442a-b4ba-8e307d5ef0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from label import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support, log_loss\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be432aa3-1796-4cdb-abdf-732bbb51bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e6315908-d98f-470a-af97-c292ec0574e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df) :\n",
    "    \"\"\"Preprocess the data.\"\"\"\n",
    "\n",
    "    select_cols = ['stump_diam', 'curb_loc', 'status', 'health', ]\n",
    "\n",
    "    df = df[select_cols]\n",
    "    df = pd.get_dummies(df, columns=['status', 'health'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b55af7b4-602d-401e-8e58-77a8e8668a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(X, y, train_size=0.7):\n",
    "    \"\"\"Generate balanced data splits.\"\"\"\n",
    "    X_train, X_, y_train, y_ = train_test_split(\n",
    "        X, y, train_size=train_size, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_, y_, train_size=0.5, stratify=y_)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad9519c5-f52a-4964-b6bf-eb71d668ce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 852.85query/s] \n",
      "Downloading: 100%|██████████| 100000/100000 [00:03<00:00, 32060.81rows/s]\n"
     ]
    }
   ],
   "source": [
    "%%bigquery df\n",
    "SELECT * FROM `bigquery-public-data.new_york_trees.tree_census_2015` LIMIT 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0dfbdce-0a51-4836-b582-b2ca340c66c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
    "df = preprocess(df, )\n",
    "label_encoder = LabelEncoder().fit(df.curb_loc)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = \\\n",
    "    get_data_splits(X=df.drop('curb_loc', axis=1), y=label_encoder.encode(df.curb_loc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6613e6db-a64c-4704-ab99-a68ebadef5be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class counts: [ 3903 66097],\n",
      "class weights: {0: 0.00025621316935690495, 1: 1.5129279694993722e-05}\n"
     ]
    }
   ],
   "source": [
    "# Class weights\n",
    "counts = np.bincount(y_train)\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (f\"class counts: {counts},\\nclass weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d14bbddf-eb77-457b-bf41-1c879462b735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | train_loss: 0.23121, val_loss: 0.22947\n",
      "Epoch: 10 | train_loss: 0.25086, val_loss: 0.25000\n",
      "Epoch: 20 | train_loss: 0.48045, val_loss: 0.46613\n",
      "Epoch: 30 | train_loss: 0.22998, val_loss: 0.22834\n",
      "Epoch: 40 | train_loss: 0.23135, val_loss: 0.23100\n",
      "Epoch: 50 | train_loss: 0.23706, val_loss: 0.23609\n",
      "Epoch: 60 | train_loss: 0.21675, val_loss: 0.21650\n",
      "Epoch: 70 | train_loss: 0.21893, val_loss: 0.21883\n",
      "Epoch: 80 | train_loss: 0.22797, val_loss: 0.22609\n",
      "Epoch: 90 | train_loss: 0.22118, val_loss: 0.21997\n"
     ]
    }
   ],
   "source": [
    "# Example: Stochastic Gradient Descent\n",
    "model = SGDClassifier(\n",
    "    loss=\"log\", penalty=\"l2\", alpha=1e-4, max_iter=1,\n",
    "    learning_rate=\"constant\", eta0=1e-1, power_t=0.1,\n",
    "    warm_start=True)\n",
    "\n",
    "# Train model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluation\n",
    "    train_loss = log_loss(y_train, model.predict_proba(X_train))\n",
    "    val_loss = log_loss(y_val, model.predict_proba(X_val))\n",
    "\n",
    "    if not epoch%10:\n",
    "        print(\n",
    "            f\"Epoch: {epoch:02d} | \"\n",
    "            f\"train_loss: {train_loss:.5f}, \"\n",
    "            f\"val_loss: {val_loss:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb5b554e-e686-4310-a452-0e9abf373249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"precision\": 0.8953372794612795,\n",
      "  \"recall\": 0.9355333333333333,\n",
      "  \"f1\": 0.9138606201580112\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n",
    "print (json.dumps(performance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8989e6e7-7ff2-4051-a38f-390d6e31d925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.joblib']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "700a5907-d987-42a1-b749-fefb2ca26ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing gs://dk-joblib/...\n",
      "Creating gs://dk-joblib/...\n",
      "Copying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "Removing file://model.joblib...]                                                \n",
      "\n",
      "Operation completed over 1 objects/1.2 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil rm -r \"gs://dk-joblib\"\n",
    "!gsutil mb -l us-central1 \"gs://dk-joblib\"\n",
    "!gsutil mv model.joblib gs://dk-joblib/model.joblib"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
