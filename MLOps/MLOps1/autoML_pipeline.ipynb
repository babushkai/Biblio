{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import os\r\n",
    "PROJECT_ID = \"\"\r\n",
    "\r\n",
    "# Get your Google Cloud project ID from gcloud\r\n",
    "if not os.getenv(\"IS_TESTING\"):\r\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\r\n",
    "    PROJECT_ID = shell_output[0]\r\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Project ID:  project-daisuke-318402\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "BUCKET_NAME=\"gs://\" + PROJECT_ID + \"-bucket\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from typing import NamedTuple\r\n",
    "\r\n",
    "import kfp\r\n",
    "from kfp import dsl\r\n",
    "from kfp.v2 import compiler\r\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\r\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\r\n",
    "from kfp.v2.google.client import AIPlatformClient\r\n",
    "\r\n",
    "from google.cloud import aiplatform\r\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "PATH=%env PATH\r\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\r\n",
    "REGION=\"us-central1\"\r\n",
    "\r\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root/\"\r\n",
    "PIPELINE_ROOT"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "env: PATH=/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'gs://project-daisuke-318402-bucket/pipeline_root/'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "@component(\r\n",
    "    base_image=\"gcr.io/deeplearning-platform-release/tf2-cpu.2-3:latest\",\r\n",
    "    output_component_file=\"tables_eval_component.yaml\", # Optional: you can use this to load the component later\r\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"],\r\n",
    ")\r\n",
    "def classif_model_eval_metrics(\r\n",
    "    project: str,\r\n",
    "    location: str,  # \"us-central1\",\r\n",
    "    api_endpoint: str,  # \"us-central1-aiplatform.googleapis.com\",\r\n",
    "    thresholds_dict_str: str,\r\n",
    "    model: Input[Model],\r\n",
    "    metrics: Output[Metrics],\r\n",
    "    metricsc: Output[ClassificationMetrics],\r\n",
    ") -> NamedTuple(\"Outputs\", [(\"dep_decision\", str)]):  # Return parameter.\r\n",
    "\r\n",
    "    \"\"\"This function renders evaluation metrics for an AutoML Tabular classification model.\r\n",
    "    It retrieves the classification model evaluation generated by the AutoML Tabular training\r\n",
    "    process, does some parsing, and uses that info to render the ROC curve and confusion matrix\r\n",
    "    for the model. It also uses given metrics threshold information and compares that to the\r\n",
    "    evaluation results to determine whether the model is sufficiently accurate to deploy.\r\n",
    "    \"\"\"\r\n",
    "    import json\r\n",
    "    import logging\r\n",
    "\r\n",
    "    from google.cloud import aiplatform\r\n",
    "\r\n",
    "    # Fetch model eval info\r\n",
    "    def get_eval_info(client, model_name):\r\n",
    "        from google.protobuf.json_format import MessageToDict\r\n",
    "\r\n",
    "        response = client.list_model_evaluations(parent=model_name)\r\n",
    "        metrics_list = []\r\n",
    "        metrics_string_list = []\r\n",
    "        for evaluation in response:\r\n",
    "            print(\"model_evaluation\")\r\n",
    "            print(\" name:\", evaluation.name)\r\n",
    "            print(\" metrics_schema_uri:\", evaluation.metrics_schema_uri)\r\n",
    "            metrics = MessageToDict(evaluation._pb.metrics)\r\n",
    "            for metric in metrics.keys():\r\n",
    "                logging.info(\"metric: %s, value: %s\", metric, metrics[metric])\r\n",
    "            metrics_str = json.dumps(metrics)\r\n",
    "            metrics_list.append(metrics)\r\n",
    "            metrics_string_list.append(metrics_str)\r\n",
    "\r\n",
    "        return (\r\n",
    "            evaluation.name,\r\n",
    "            metrics_list,\r\n",
    "            metrics_string_list,\r\n",
    "        )\r\n",
    "\r\n",
    "    # Use the given metrics threshold(s) to determine whether the model is \r\n",
    "    # accurate enough to deploy.\r\n",
    "    def classification_thresholds_check(metrics_dict, thresholds_dict):\r\n",
    "        for k, v in thresholds_dict.items():\r\n",
    "            logging.info(\"k {}, v {}\".format(k, v))\r\n",
    "            if k in [\"auRoc\", \"auPrc\"]:  # higher is better\r\n",
    "                if metrics_dict[k] < v:  # if under threshold, don't deploy\r\n",
    "                    logging.info(\r\n",
    "                        \"{} < {}; returning False\".format(metrics_dict[k], v)\r\n",
    "                    )\r\n",
    "                    return False\r\n",
    "        logging.info(\"threshold checks passed.\")\r\n",
    "        return True\r\n",
    "\r\n",
    "    def log_metrics(metrics_list, metricsc):\r\n",
    "        test_confusion_matrix = metrics_list[0][\"confusionMatrix\"]\r\n",
    "        logging.info(\"rows: %s\", test_confusion_matrix[\"rows\"])\r\n",
    "\r\n",
    "        # log the ROC curve\r\n",
    "        fpr = []\r\n",
    "        tpr = []\r\n",
    "        thresholds = []\r\n",
    "        for item in metrics_list[0][\"confidenceMetrics\"]:\r\n",
    "            fpr.append(item.get(\"falsePositiveRate\", 0.0))\r\n",
    "            tpr.append(item.get(\"recall\", 0.0))\r\n",
    "            thresholds.append(item.get(\"confidenceThreshold\", 0.0))\r\n",
    "        print(f\"fpr: {fpr}\")\r\n",
    "        print(f\"tpr: {tpr}\")\r\n",
    "        print(f\"thresholds: {thresholds}\")\r\n",
    "        metricsc.log_roc_curve(fpr, tpr, thresholds)\r\n",
    "\r\n",
    "        # log the confusion matrix\r\n",
    "        annotations = []\r\n",
    "        for item in test_confusion_matrix[\"annotationSpecs\"]:\r\n",
    "            annotations.append(item[\"displayName\"])\r\n",
    "        logging.info(\"confusion matrix annotations: %s\", annotations)\r\n",
    "        metricsc.log_confusion_matrix(\r\n",
    "            annotations,\r\n",
    "            test_confusion_matrix[\"rows\"],\r\n",
    "        )\r\n",
    "\r\n",
    "        # log textual metrics info as well\r\n",
    "        for metric in metrics_list[0].keys():\r\n",
    "            if metric != \"confidenceMetrics\":\r\n",
    "                val_string = json.dumps(metrics_list[0][metric])\r\n",
    "                metrics.log_metric(metric, val_string)\r\n",
    "        # metrics.metadata[\"model_type\"] = \"AutoML Tabular classification\"\r\n",
    "\r\n",
    "    logging.getLogger().setLevel(logging.INFO)\r\n",
    "    aiplatform.init(project=project)\r\n",
    "    # extract the model resource name from the input Model Artifact\r\n",
    "    model_resource_path = model.uri.replace(\"aiplatform://v1/\", \"\")\r\n",
    "    logging.info(\"model path: %s\", model_resource_path)\r\n",
    "\r\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\r\n",
    "    # Initialize client that will be used to create and send requests.\r\n",
    "    client = aiplatform.gapic.ModelServiceClient(client_options=client_options)\r\n",
    "    eval_name, metrics_list, metrics_str_list = get_eval_info(\r\n",
    "        client, model_resource_path\r\n",
    "    )\r\n",
    "    logging.info(\"got evaluation name: %s\", eval_name)\r\n",
    "    logging.info(\"got metrics list: %s\", metrics_list)\r\n",
    "    log_metrics(metrics_list, metricsc)\r\n",
    "\r\n",
    "    thresholds_dict = json.loads(thresholds_dict_str)\r\n",
    "    deploy = classification_thresholds_check(metrics_list[0], thresholds_dict)\r\n",
    "    if deploy:\r\n",
    "        dep_decision = \"true\"\r\n",
    "    else:\r\n",
    "        dep_decision = \"false\"\r\n",
    "    logging.info(\"deployment decision is %s\", dep_decision)\r\n",
    "\r\n",
    "    return (dep_decision,)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import time\r\n",
    "DISPLAY_NAME = 'automl-beans{}'.format(str(int(time.time())))\r\n",
    "print(DISPLAY_NAME)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "automl-beans1631088924\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "@kfp.dsl.pipeline(name=\"automl-tab-beans-training-v2\",\r\n",
    "                  pipeline_root=PIPELINE_ROOT)\r\n",
    "def pipeline(\r\n",
    "    bq_source: str = \"bq://aju-dev-demos.beans.beans1\",\r\n",
    "    display_name: str = DISPLAY_NAME,\r\n",
    "    project: str = PROJECT_ID,\r\n",
    "    gcp_region: str = \"us-central1\",\r\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\r\n",
    "    thresholds_dict_str: str = '{\"auRoc\": 0.95}',\r\n",
    "):\r\n",
    "    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\r\n",
    "        project=project, display_name=display_name, bq_source=bq_source\r\n",
    "    )\r\n",
    "\r\n",
    "    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\r\n",
    "        project=project,\r\n",
    "        display_name=display_name,\r\n",
    "        optimization_prediction_type=\"classification\",\r\n",
    "        budget_milli_node_hours=1000,\r\n",
    "        column_transformations=[\r\n",
    "            {\"numeric\": {\"column_name\": \"Area\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"Perimeter\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"MajorAxisLength\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"MinorAxisLength\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"AspectRation\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"Eccentricity\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"ConvexArea\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"EquivDiameter\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"Extent\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"Solidity\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"roundness\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"Compactness\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor1\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor2\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor3\"}},\r\n",
    "            {\"numeric\": {\"column_name\": \"ShapeFactor4\"}},\r\n",
    "            {\"categorical\": {\"column_name\": \"Class\"}},\r\n",
    "        ],\r\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\r\n",
    "        target_column=\"Class\",\r\n",
    "    )\r\n",
    "    model_eval_task = classif_model_eval_metrics(\r\n",
    "        project,\r\n",
    "        gcp_region,\r\n",
    "        api_endpoint,\r\n",
    "        thresholds_dict_str,\r\n",
    "        training_op.outputs[\"model\"],\r\n",
    "    )\r\n",
    "\r\n",
    "    with dsl.Condition(\r\n",
    "        model_eval_task.outputs[\"dep_decision\"] == \"true\",\r\n",
    "        name=\"deploy_decision\",\r\n",
    "    ):\r\n",
    "\r\n",
    "        deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\r\n",
    "            model=training_op.outputs[\"model\"],\r\n",
    "            project=project,\r\n",
    "            machine_type=\"n1-standard-4\",\r\n",
    "        )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "compiler.Compiler().compile(\r\n",
    "    pipeline_func=pipeline, package_path=\"tab_classif_pipeline.json\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "response = api_client.create_run_from_job_spec(\r\n",
    "    \"tab_classif_pipeline.json\", pipeline_root=PIPELINE_ROOT,\r\n",
    "    parameter_values={\"project\": PROJECT_ID,\r\n",
    "                      \"display_name\": DISPLAY_NAME}\r\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'api_client' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1425/3562225362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = api_client.create_run_from_job_spec(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"tab_classif_pipeline.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPELINE_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     parameter_values={\"project\": PROJECT_ID,\n\u001b[1;32m      4\u001b[0m                       \"display_name\": DISPLAY_NAME}\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api_client' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m78",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m78"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}