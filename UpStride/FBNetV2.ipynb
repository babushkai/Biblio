{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "three-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "knowing-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables needed for softmax gumbel computation in MaskConv\n",
    "temperature = 5.0  # should be multiply by 0.956 at the end of every epoch, see section 4.1 in the paper\n",
    "\n",
    "\n",
    "def define_temperature(new_temperature):\n",
    "  global temperature\n",
    "  temperature = new_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indoor-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_vector(channel_sizes: List[int], dtype) -> List[tf.Tensor]:\n",
    "  \"\"\"this function return a list of vector with ones at the beginning and zeros at the end\n",
    "  it uses numpy because there is no reason for these operations to be inside the tensorflow graph.\n",
    "  Args:\n",
    "      channel_sizes (List[int]): number of channels in the convolution\n",
    "  Returns:\n",
    "      List[tf.Tensor]: list of vector like [1., 1., 1., 0., 0., 0.]\n",
    "  \"\"\"\n",
    "  binary_vectors = []\n",
    "  max_size = channel_sizes[-1]\n",
    "  for i in range(len(channel_sizes)):\n",
    "    ones = np.ones(channel_sizes[i])\n",
    "    zeros = np.zeros(max_size - channel_sizes[i])\n",
    "    binary_vectors.append(tf.convert_to_tensor(np.concatenate([ones, zeros], 0), dtype=dtype))\n",
    "  return binary_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "suited-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, gumble_noise=False):\n",
    "  \"\"\"please have a look at https://arxiv.org/pdf/1611.01144.pdf for gumble definition\n",
    "  \"\"\"\n",
    "  global temperature\n",
    "\n",
    "  if gumble_noise:\n",
    "    # Gumble distribution -log(-log(u)), where u ~ (0,1) is a uniform distribution and\n",
    "    # must be sampled from the open-interval `(0, 1)` but tf.random.uniform generates samples\n",
    "    #  where The lower bound minval is included in the range like [0, 1). To make sure the range\n",
    "    # to be (0, 1), np.finfo(float).tiny is used as minval which gives a tiny postive floating point number\n",
    "    u = tf.random.uniform(minval=np.finfo(float).tiny, maxval=1.0, shape=tf.shape(logits))\n",
    "    noise = -tf.math.log(-tf.math.log(u))  # Noise from gumbel distribution\n",
    "  else:\n",
    "    noise = 0.0001\n",
    "  # During mixed precision training, Weight Variable data type is inferred from \"inputs\" during call method\n",
    "  # This makes alpha to be converted to float16. \n",
    "  # Since we are computing softmax at the end, we need to convert logits(alpha) to float32\n",
    "  logits = tf.cast(logits, tf.float32) \n",
    "  noisy_logits = (noise + logits) / temperature\n",
    "\n",
    "  return tf.math.softmax(noisy_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ancient-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(binary_vectors: List[tf.Tensor], g: List[float]):\n",
    "  vectors = [g[i] * binary_vectors[i] for i in range(g.shape[0])]\n",
    "  vectors = tf.stack(vectors, axis=0)\n",
    "  vector = tf.reduce_sum(vectors, axis=0)\n",
    "  return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "charged-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelMasking(tf.keras.layers.Layer):\n",
    "  def __init__(self, min: int, max: int, step: int, name: str, gumble_noise=True, regularizer=None):\n",
    "    super().__init__(name=name)\n",
    "    self.min = min\n",
    "    self.max = max\n",
    "    self.step = step\n",
    "    self.channel_sizes = []\n",
    "    self.gumble_noise = gumble_noise\n",
    "    self.regularizer = regularizer\n",
    "    for i in range(self.min, self.max+1, self.step):\n",
    "      self.channel_sizes.append(i)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.alpha = self.add_weight(name=f\"alpha\",\n",
    "                                 shape=(len(self.channel_sizes),),\n",
    "                                 initializer=tf.keras.initializers.Constant(value=1.), regularizer=self.regularizer)\n",
    "    self.binary_vectors = create_binary_vector(self.channel_sizes, dtype=self.alpha.dtype)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    self.g = gumbel_softmax(self.alpha, self.gumble_noise)\n",
    "    mask = get_mask(self.binary_vectors,  self.g)\n",
    "    # Convert mast from Float32 to Float16 during mixed precision. \n",
    "    mask = tf.cast(mask, dtype=inputs.dtype)\n",
    "\n",
    "    # work with channel last but not channel first\n",
    "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "      mask = tf.reshape(mask, [1, self.channel_sizes[-1], 1, 1])\n",
    "    if type(inputs) == list:\n",
    "      return [mask * inputs[i] for i in range(len(inputs))]\n",
    "    else:\n",
    "      return mask * inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accessible-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(initial_value, decay_steps, decay_rate):\n",
    "  \"\"\"\n",
    "          Applies exponential decay to initial value\n",
    "       Args:\n",
    "          initial_value: The initial learning value\n",
    "          decay_steps: Number of steps to decay over\n",
    "          decay_rate: decay rate\n",
    "      \"\"\"\n",
    "  return lambda step: initial_value * decay_rate ** (step / decay_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "suspended-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trainable_weights(model, arch_params_name='alpha'):\n",
    "  \"\"\"\n",
    "      split the model parameters  in weights and architectural params\n",
    "  \"\"\"\n",
    "  weights = []\n",
    "  arch_params = []\n",
    "  for trainable_weight in model.trainable_variables:\n",
    "    if arch_params_name in trainable_weight.name:\n",
    "      arch_params.append(trainable_weight)\n",
    "    else:\n",
    "      weights.append(trainable_weight)\n",
    "  if not arch_params:\n",
    "    raise ValueError(f\"No architecture parameters found by the name {arch_params_name}\")\n",
    "  return weights, arch_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "north-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_training_analysis(model, saved_file_path):\n",
    "  layer_name = ''\n",
    "  saved_file_content = {}\n",
    "  for layer in model.layers:\n",
    "    # if type(layer) == tf.keras.Conv2D:\n",
    "    #     layer_name = layer.name\n",
    "    if type(layer) == ChannelMasking and layer.name[-8:] == '_savable':\n",
    "      layer_name = layer.name[:-8]\n",
    "      max_alpha_id = int(tf.math.argmax(layer.alpha).numpy())\n",
    "      value = layer.min + max_alpha_id * layer.step\n",
    "      saved_file_content[layer_name] = value\n",
    "  print(saved_file_content)\n",
    "  with open(saved_file_path, 'w') as f:\n",
    "    yaml.dump(saved_file_content, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rental-shooting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_arch_params(model, epoch, log_dir):\n",
    "  json_file_path = os.path.join(log_dir, f'alpha.json')\n",
    "  content = {}\n",
    "  if os.path.exists(json_file_path):\n",
    "    with open(json_file_path) as f:\n",
    "      content = json.load(f)\n",
    "  for layer in model.layers:\n",
    "    if type(layer) == ChannelMasking:\n",
    "      # need to convert from numpy.float32 to pure python float32 to prepare the dumps\n",
    "      if str(epoch) not in content:\n",
    "        content[str(epoch)] = {}\n",
    "      content[str(epoch)][layer.name] = list(map(float, layer.alpha.numpy()))\n",
    "  with open(json_file_path, 'w') as f:\n",
    "    f.write(json.dumps(content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
