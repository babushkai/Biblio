{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LST4kpCv8L59",
        "vOI_v_eq8L6D",
        "zEyau_uJ8L6G",
        "JE-N9Iu88L6I",
        "zIKCH1S18L6I"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UobyVUNx8L5u"
      },
      "source": [
        "# 深層強化学習講義 演習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB--Lr3u8L50"
      },
      "source": [
        "## 目次\n",
        "**課題：様々な強化学習アルゴリズムを実装して学習させてみましょう**\n",
        "1. シミュレータ環境の構築\n",
        "2. 価値に基づく手法 (Value-based Methods)\n",
        "  - 2.1 テーブル解法\n",
        "    - 2.1.1 SARSA\n",
        "    - 2.1.2 Q-Learning\n",
        "  - 2.2 NNによる価値関数の近似\n",
        "    - 2.2.1 Deep Q-Network（DQN）\n",
        "3. 方策に基づく手法 (Policy-based Methods)\n",
        "  - 3.1 方策勾配法：方策の近似\n",
        "    - 3.1.1 REINFORCE\n",
        "  - 3.2 Actor-Critic\n",
        "    - 3.2.1 Actor-Criticの実装例\n",
        "  - 3.3 連続値行動空間\n",
        "    - 3.3.1 連続値行動空間の環境の例：pendulum-v0\n",
        "    - 3.3.2 Deep Deterministic Policy Gradient (DDPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhocjGbq8L51"
      },
      "source": [
        "**今回の講義の演習に関する注意事項**\n",
        "- 実行時にメモリをたくさん使用するので，colabだと途中で実行が止まってしまうことがあります．\n",
        "- その場合は，ランタイムを再起動して，以下の3つのセルを実行してから目的のセルを再実行してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvyMhykL8L52"
      },
      "source": [
        "# Colab上では以下を実行してください\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install JSAnimation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i7R39On8L52"
      },
      "source": [
        "# Colab上では以下を実行してください\n",
        "from pyvirtualdisplay import Display\n",
        "pydisplay = Display(visible=0, size=(400, 300))\n",
        "pydisplay.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV6X-m9d8L53"
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import wrappers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical, Normal\n",
        "import matplotlib\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "from JSAnimation.IPython_display import display_animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szPVE3gz8L54"
      },
      "source": [
        "## 1. シミュレータ環境の構築\n",
        "OpenAI Gymと呼ばれる，強化学習のシミュレータのライブラリを用いて環境を作成します．\n",
        "- 今回は，`CartPole-v0`と呼ばれる，台車に振子がついた環境を利用します（[詳細](https://github.com/openai/gym/wiki/CartPole-v0)）．\n",
        "    - 状態空間：4つの連続値\n",
        "        1. カートの座標\n",
        "        1. カートの速度\n",
        "        1. ポールの角度\n",
        "        1. ポールの角速度\n",
        "    - 行動空間：1つの離散値．\n",
        "      - 左に押す(0) または 右に押す(1)\n",
        "    - 以下の終端条件のいずれかを満たすとエピソードが終了します．\n",
        "      - 棒の角度が±12°より傾いたとき\n",
        "      - 台車の位置が±2.4の範囲を逸脱したとき\n",
        "      - エピソードの長さが200ステップに達したとき\n",
        "    - 報酬：終端条件に達するまで常に+1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8AvjRdH8L54"
      },
      "source": [
        "環境を作成してランダムな行動を取ってみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l1FaBu38L55"
      },
      "source": [
        "env = gym.make('CartPole-v0')  # シミュレータ環境の構築\n",
        "frames = []\n",
        "for episode in range(5):\n",
        "    state = env.reset()  # エピソードを開始（環境の初期化）\n",
        "    env.render()  # シミュレータ画面の出力\n",
        "    screen = env.render(mode='rgb_array')  # notebook上での結果の可視化用\n",
        "    frames.append(screen)\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # 行動をランダムに選択\n",
        "        next_state, reward, done, _ = env.step(action)  # 行動を実行し、次の状態、 報酬、 終端か否かの情報を取得\n",
        "        env.render()\n",
        "        screen = env.render(mode='rgb_array')\n",
        "        frames.append(screen)\n",
        "env.close()  # 画面出力の終了"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjTcbhCa8L55"
      },
      "source": [
        "以下のセルを実行すると，notebook上で結果が可視化できます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dVWzhgq8L56"
      },
      "source": [
        "# 結果の確認\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0BIav8f8L57"
      },
      "source": [
        "**やってみよう！（余力のある人向けの追加課題）**\n",
        "1. OpenAI Gymには，他にも様々なシミュレータ・ゲーム環境が用意されています．それらの環境をインポートして，ランダムな行動をとったときの結果を可視化してみましょう．\n",
        "    - 環境の一覧は，OpenAI Gymの[webサイト](https://gym.openai.com/envs/)や[Github wiki](https://github.com/openai/gym/wiki/Table-of-environments)に掲載されています．\n",
        "      - ライブラリを追加でインストールすることが必要な環境がありますが，今回その詳細については扱いません．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR4fVbUz8L57"
      },
      "source": [
        "# LET'S TRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JmjKEBe8L57"
      },
      "source": [
        "## 2.価値に基づく手法 (Value-based Methods)\n",
        "価値に基づく手法では，価値に基づいて行動を決定する方策を利用します．\n",
        "- 方策$\\pi$として，小さい確率$\\epsilon$で一様な確率でランダムな行動を選択し，それ以外は最もQ値（の推定値）が最も高い行動を選択する，**ε-greedy方策**が用いられることが多いです．\n",
        "  - ランダムな行動を選択することで，探索を促進するために利用されます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4o4KWTW8L58"
      },
      "source": [
        "今回は，価値に基づく代表的な手法として，**SARSA**と**Q-learning**を扱います．\n",
        "- どちらも以下の再帰的な更新により，Q関数を推定します，$$Q^{\\pi}(s_t,a_t) \\leftarrow Q^{\\pi}(s_t,a_t)+\\alpha \\delta_t$$$$\\delta_t = y_t - Q^{\\pi}(s_t,a_t)$$\n",
        "  - これは，Q値を目標値$y_t$に向かって更新する操作になっています．\n",
        "  - $\\alpha$：**学習率**（ステップサイズ）\n",
        "  - $\\delta_t$：**TD誤差** (temporal difference error)\n",
        "  - $y_t$：**TDターゲット**\n",
        "SARSAとQ-learningでTDターゲット$y_t$の求め方が異なります．$$y^{SARSA}_t = r_{t+1}+\\gamma Q^{\\pi}(s_{t+1},a_{t+1})$$$$y^{Q-learning}_t = r_{t+1}+\\gamma \\max_{a'}Q^{\\pi}(s_{t+1},a')$$\n",
        "  - $\\gamma$：**割引率**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Nwbsrz8L58"
      },
      "source": [
        "### 2.1 テーブル解法\n",
        "- cartpoleは連続値をとる状態空間の問題設定です．テーブル解法では，本来は連続値として表現される状態空間を適当な間隔で区切って離散化します．\n",
        "- ここでは，状態空間の各次元を6分割して， $6^4\\times2=2692$個の値を持つ表としてQ関数を表現します．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P7pE3qc8L58"
      },
      "source": [
        "まずは，そのために便利な関数を作っておきましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5QdY5-k8L59"
      },
      "source": [
        "def bins(clip_min, clip_max, num):\n",
        "    return np.linspace(clip_min, clip_max, num+1)[1:-1]\n",
        "\n",
        "# 状態を離散化して対応するインデックスを返す関数（binの上限・下限はcartpole環境固有のものを用いています）\n",
        "def discretize_state(observation, num_discretize):\n",
        "    c_pos, c_v, p_angle, p_v = observation\n",
        "    discretized = [\n",
        "        np.digitize(c_pos, bins=bins(-2.4, 2.4, num_discretize)), \n",
        "        np.digitize(c_v, bins=bins(-3.0, 3.0, num_discretize)),\n",
        "        np.digitize(p_angle, bins=bins(-0.5, 0.5, num_discretize)),\n",
        "        np.digitize(p_v, bins=bins(-2.0, 2.0, num_discretize))\n",
        "    ]\n",
        "    return sum([x*(num_discretize**i) for i, x in enumerate(discretized)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LST4kpCv8L59"
      },
      "source": [
        "#### 2.1.1 SARSA\n",
        "SARSAのQ関数に関する要素は以下の通りです．\n",
        "- 価値の更新：$Q^{\\pi}(s_t,a_t) \\leftarrow Q^{\\pi}(s_t,a_t)+\\alpha \\delta_t$\n",
        "- TD誤差：$\\delta_t = y^{SARSA}_t - Q^{\\pi}(s_t,a_t)$\n",
        "- TDターゲット：$y^{SARSA}_t = r_{t+1}+\\gamma Q^{\\pi}(s_{t+1},a_{t+1})$\n",
        "それぞれ代入するとSARSAの価値の更新式は$$Q^{\\pi}(s_t,a_t) \\leftarrow Q^{\\pi}(s_t,a_t)+\\alpha \\left(r_{t+1}+\\gamma Q^{\\pi}(s_{t+1},a_{t+1}) -Q^{\\pi}(s_t,a_t)\\right)$$となります，\n",
        "\n",
        "方策は**ε-greedy方策**を用いてみましょう．\n",
        "  - ランダムな行動をとる$\\epsilon$は，定数でないこともよくあります．\n",
        "  - 学習が進むごとに減衰させることで，学習初期は探索を促し，終盤は活用を促すように設計することも多いです．\n",
        "  - 学習した方策の性能のテストをするときは，常にQ値が最大の行動をとるようにします．\n",
        "\n",
        "**補足説明**\n",
        "- SARSAでは，Q値の更新に利用する行動$a$は，方策$\\pi$から得られたものであるので，**on-policy（方策オン）**の手法です．\n",
        "  - on-policyの手法では，方策を制御に用いる一方で，同じ方策を用いて方策の価値を推定します．\n",
        "  - Q-learning（off-policy）の価値の更新式と比較すると違いが明確になります．\n",
        "- この実装ではエピソードが途中で終了した場合はペナルティを本来の報酬から引いています\n",
        "  - 後に述べるように．テーブルを用いてQ関数を表現した場合，学習効率が悪く不安定になりがちなためです．\n",
        "  - このような，学習を容易にするための追加的な報酬の設計を**reward shaping**といいます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VjQEeu08L59"
      },
      "source": [
        "class SarsaAgent:\n",
        "    def __init__(self, num_state, num_action, num_discretize, gamma=0.99, alpha=0.5, max_initial_q=0.1):\n",
        "        self.num_action = num_action\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.alpha = alpha  # 学習率\n",
        "        # Qテーブルを作成し乱数で初期化\n",
        "        self.qtable = np.random.uniform(low=-max_initial_q, high=max_initial_q, size=(num_discretize**num_state, num_action)) \n",
        "    \n",
        "    # Qテーブルを更新\n",
        "    def update_qtable(self, state, action, reward, next_state, next_action):\n",
        "        self.qtable[state, action] += self.alpha*(reward+self.gamma*self.qtable[next_state, next_action]-self.qtable[state, action])\n",
        "    \n",
        "    # Q値が最大の行動を選択\n",
        "    def get_greedy_action(self, state):\n",
        "        action = np.argmax(self.qtable[state])\n",
        "        return action\n",
        "    \n",
        "    # ε-greedyに行動を選択\n",
        "    def get_action(self, state, episode):\n",
        "        epsilon = 0.7 * (1/(episode+1))  # ここでは0.5から減衰していくようなεを設定\n",
        "        if epsilon <= np.random.uniform(0,1):\n",
        "            action = self.get_greedy_action(state)\n",
        "        else:\n",
        "            action = np.random.choice(self.num_action)\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkJZ6grZ8L59"
      },
      "source": [
        "# 各種設定\n",
        "num_episode =1200  # 学習エピソード数\n",
        "penalty = 10  # 途中でエピソードが終了したときのペナルティ\n",
        "num_discretize = 6  # 状態空間の分割数\n",
        "# ログ用の設定\n",
        "episode_rewards = []\n",
        "num_average_epidodes = 10\n",
        "\n",
        "# エージェントの学習\n",
        "env = gym.make('CartPole-v0')\n",
        "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
        "agent = SarsaAgent(env.observation_space.shape[0], env.action_space.n, num_discretize)\n",
        "for episode in range(num_episode):\n",
        "    observation = env.reset()  # envからは4次元の連続値の観測が帰ってくる\n",
        "    state = discretize_state(observation, num_discretize)  # 観測の離散化（状態のインデックスを取得）\n",
        "    action = agent.get_action(state, episode)  #  行動を選択\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        # もしエピソードの途中で終了してしまったらペナルティを加える\n",
        "        if done and t < max_steps - 1:\n",
        "            reward = - penalty\n",
        "        episode_reward += reward\n",
        "        next_state = discretize_state(observation, num_discretize)\n",
        "        next_action = agent.get_action(next_state, episode)\n",
        "        agent.update_qtable(state, action, reward, next_state, next_action)  # Q値の表を更新\n",
        "        state, action = next_state, next_action\n",
        "        if done:\n",
        "            break\n",
        "    episode_rewards.append(episode_reward)\n",
        "    if episode % 50 == 0:\n",
        "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "            \n",
        "# 学習途中の累積報酬の移動平均を表示\n",
        "moving_average = np.convolve(episode_rewards, np.ones(num_average_epidodes)/num_average_epidodes, mode='valid')\n",
        "plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "plt.title('SARSA: average rewards in %d episodes' % num_average_epidodes)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuUON6jL8L5-"
      },
      "source": [
        "エピソードで得られた合計報酬が改善していき，方策が学習できていることがわかります．\n",
        "- 注意：初期値などの設定より，学習が非常に不安定になる場合があります．\n",
        "\n",
        "それでは，得られた方策をテストして結果の動画を表示してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbrBC7Fa8L5-"
      },
      "source": [
        "# 最終的に得られた方策のテスト（可視化）\n",
        "env = gym.make('CartPole-v0')\n",
        "frames = []\n",
        "for episode in range(5):\n",
        "    observation = env.reset()\n",
        "    state = discretize_state(observation, num_discretize)\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_greedy_action(state)\n",
        "        next_observation, reward, done, _ = env.step(action)\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        state = discretize_state(next_observation, num_discretize)\n",
        "env.close()\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnQ6pSbN8L5-"
      },
      "source": [
        "ランダムな行動を選択したときよりも，長い間立て続けられていると思います．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LH-nKY_8L5-"
      },
      "source": [
        "**やってみよう！（余力のある人向けの追加課題）**\n",
        "1. 上記のプログラムには複数のハイパーパラメータがあります．それらを変化させて，学習曲線がどのように変化するかを観察してみましょう．\n",
        "    - ハイパーパラメータの例\n",
        "      - `num_discretize`：離散化する数\n",
        "      - `gamma`：割引率\n",
        "      - `alpha`：学習率\n",
        "      - `penalty`：途中で学習が終了したときのペナルティの大きさ\n",
        "      - `epsilon`：ε-greedy方策でのεの関数（例えば，定数にしてみる，線形に減衰させてみる，など）\n",
        "      - `num_episode`：学習エピソード数\n",
        "      - `max_initial_q`：Qテーブルの初期化に用いる乱数の最大値"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R4cFnyC8L5_"
      },
      "source": [
        "# LET'S TRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucGcdU4s8L5_"
      },
      "source": [
        "#### 2.1.2 Q-Learning\n",
        "Q-learningのQ関数に関する要素は以下の通りです．\n",
        "- 価値の更新：$Q^{\\pi}(s_t,a_t) \\leftarrow Q^{\\pi}(s_t,a_t)+\\alpha \\delta_t$\n",
        "- TD誤差：$\\delta_t = y^{Q-learning}_t - Q^{\\pi}(s_t,a_t)$\n",
        "- TDターゲット：$y^{Q-learning}_t = r_{t+1}+\\gamma \\max_{a'}Q^{\\pi}(s_{t+1},a')$\n",
        "\n",
        "それぞれ代入するとQ-learningの価値の更新式は$$Q^{\\pi}(s_t,a_t) \\leftarrow Q^{\\pi}(s_t,a_t)+\\alpha \\left(r_{t+1}+\\gamma \\max_{a'}Q^{\\pi}(s_{t+1},a') -Q^{\\pi}(s_t,a_t)\\right)$$となります，\n",
        "\n",
        "**補足説明**\n",
        "- Q-learningでは，Q値の更新に利用する行動$a$は，Q関数が最大となる行動を決定的に選択するので，**off-policy（方策オフ）**の手法です．\n",
        "  - off-policyの手法では，学習時の制御に用いる方策（**挙動方策**）と，評価され改善される方策（**推定方策，ターゲット方策**）で異なるものを用います．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbcM0x2I8L6A"
      },
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, num_state, num_action, num_discretize, gamma=0.99, alpha=0.5, max_initial_q=0.1):\n",
        "        self.num_action = num_action\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.alpha = alpha  # 学習率\n",
        "        # Qテーブルを作成し乱数で初期化\n",
        "        self.qtable = np.random.uniform(low=-max_initial_q, high=max_initial_q, size=(num_discretize**num_state, num_action))\n",
        "\n",
        "    # Qテーブルを更新\n",
        "    def update_qtable(self, state, action, reward, next_state):\n",
        "        self.qtable[state, action] \\\n",
        "            += self.alpha*(reward+self.gamma*self.qtable[next_state, np.argmax(self.qtable[next_state])] - self.qtable[state, action])\n",
        "    \n",
        "    # Q値が最大の行動を選択\n",
        "    def get_greedy_action(self, state):\n",
        "        action = np.argmax(self.qtable[state])\n",
        "        return action\n",
        "    \n",
        "    # ε-greedyに行動を選択\n",
        "    def get_action(self, state, episode):\n",
        "        epsilon = 0.7 * (1/(episode+1))  # ここでは0.5から減衰していくようなεを設定\n",
        "        if epsilon <= np.random.uniform(0,1):\n",
        "            action = self.get_greedy_action(state)\n",
        "        else:\n",
        "            action = np.random.choice(self.num_action)\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2YiNAoy8L6A"
      },
      "source": [
        "# 各種設定\n",
        "num_episode =1200  # 学習エピソード数\n",
        "penalty = 10  # 途中でエピソードが終了したときのペナルティ\n",
        "num_discretize = 6  # 状態空間の分割数\n",
        "\n",
        "# ログ\n",
        "episode_rewards = []\n",
        "num_average_epidodes = 10\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
        "\n",
        "agent = QLearningAgent(env.observation_space.shape[0], env.action_space.n, num_discretize)\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    observation = env.reset()  # envからは4次元の連続値の観測が返ってくる\n",
        "    state = discretize_state(observation, num_discretize)  # 観測の離散化（状態のインデックスを取得）\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        action = agent.get_action(state, episode)  #  行動を選択\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        # もしエピソードの途中で終了してしまったらペナルティを加える\n",
        "        if done and t < max_steps - 1:\n",
        "            reward = - penalty\n",
        "        episode_reward += reward\n",
        "        next_state = discretize_state(observation, num_discretize)\n",
        "        agent.update_qtable(state, action, reward, next_state)  # Q値の表を更新\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    episode_rewards.append(episode_reward)\n",
        "    if episode % 50 == 0:\n",
        "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "\n",
        "# 累積報酬の移動平均を表示\n",
        "moving_average = np.convolve(episode_rewards, np.ones(num_average_epidodes)/num_average_epidodes, mode='valid')\n",
        "plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "plt.title('Q-Learning: average rewards in %d episodes' % num_average_epidodes)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pjzSyrm8L6A"
      },
      "source": [
        "# 最終的に得られた方策のテスト（可視化）\n",
        "env = gym.make('CartPole-v0')\n",
        "frames = []\n",
        "for episode in range(5):\n",
        "    observation = env.reset()\n",
        "    state = discretize_state(observation, num_discretize)\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_greedy_action(state)\n",
        "        next_observation, reward, done, _ = env.step(action)\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "        state = discretize_state(next_observation, num_discretize)\n",
        "env.close()\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geE5h-iN8L6A"
      },
      "source": [
        "**やってみよう！（余力のある人向けの追加課題）**\n",
        "1. SARSAのときと同様に，Q-learningにもハイパーパラメータが複数あります．それらを変化させた場合，学習曲線がどのように変化するかを観察してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLtuoQ-o8L6B"
      },
      "source": [
        "# LET'S TRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlDlEilO8L6B"
      },
      "source": [
        "### 2.2 NNによる価値関数の近似\n",
        "これまでは，連続値の状態空間を持つ問題に対して，価値関数に基づく手法で状態空間を離散化して，価値関数をテーブルとして表現していました．\n",
        "\n",
        "ここでは，ニューラルネットワークを用いて価値関数を近似することを考えます\n",
        "- 状態空間の離散化を行わずに連続値のまま扱うことができます"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY6KQWbI8L6B"
      },
      "source": [
        "#### 2.2.1 Deep Q-Network（DQN）\n",
        "DQN（Deep Q-Network）は，Q関数をNNによって近似した手法です．Q-learningのTD誤差は，$$\\delta_t = r_{t+1}+\\gamma \\max_{a'}Q^{\\pi}(s_{t+1},a') - Q^{\\pi}(s_t,a_t)$$であるので，$r_{t+1}+\\gamma \\max_{a'}Q^{\\pi}(s_{t+1},a')$と$Q^{\\pi}(s_t,a_t)$の差の最小化をすればいいことがわかります．\n",
        "- 今回の実装ではMSEを用いています．\n",
        "\n",
        "**特徴**\n",
        "- **経験リプレイ**（experience replay）の利用\n",
        "  - **リプレイバッファ**（replay buffer）にこれまでの状態遷移を記録しておき，そこからサンプルすることでQ関数のミニバッチ学習をします．\n",
        "- **固定したターゲットネットワーク**（fixed target Q-network）の利用\n",
        "  - 学習する対象のQ関数$Q^{\\pi}(s_t,a_t)$も，更新によって近づける目標値$r_{t+1}+\\gamma \\max_{a'}Q^{\\pi}(s_{t+1},a')$もどちらも同じパラメータを持つQ関数を利用しています．そのため，そのまま勾配法による最適化を行うと，元のQ値と目標値の両方が更新されてしまうことになります．\n",
        "  - これを避けるために，目標値$r_{t+1}+\\gamma \\max_{a'}Q^{\\pi}(s_{t+1},a')$は固定した上でQ関数の最適化を行います．\n",
        "    - 実装上は，目標値側のQ関数の勾配の情報を削除して数値として扱います．\n",
        "\n",
        "**論文**\n",
        "- [Human-level control through deep reinforcement learning （Nature版）](https://www.nature.com/articles/nature14236)\n",
        "- [Playing Atari with Deep Reinforcement Learning （NIPS2013 workshop版）](https://arxiv.org/abs/1312.5602)\n",
        "\n",
        "\n",
        "**補足説明**\n",
        "- 深層強化学習には様々な実装の仕方がありますが，今回の講義では以下のモジュールに分けて実装しています．\n",
        "  - 1つ目のセル：関数近似のためのニューラルネットワーク（とリプレイバッファ）\n",
        "  - 2つ目のセル：エージェントの定義\n",
        "  - 3つ目のセル：実際に環境と相互作用して学習を実行する部分\n",
        "  - (4つ目のセル：学習したエージェントの結果の可視化）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtHT5-Mp8L6B"
      },
      "source": [
        "# Q関数の定義\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, num_state, num_action, hidden_size=16):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_state, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc4 = nn.Linear(hidden_size, num_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.elu(self.fc1(x))\n",
        "        h = F.elu(self.fc2(h))\n",
        "        h = F.elu(self.fc3(h))\n",
        "        y = F.elu(self.fc4(h))\n",
        "        return y\n",
        "\n",
        "# リプレイバッファの定義\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, memory_size):\n",
        "        self.memory_size = memory_size\n",
        "        self.memory = deque([], maxlen = memory_size)\n",
        "    \n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size)\n",
        "        states      = np.array([self.memory[index]['state'] for index in batch_indexes])\n",
        "        next_states = np.array([self.memory[index]['next_state'] for index in batch_indexes])\n",
        "        rewards     = np.array([self.memory[index]['reward'] for index in batch_indexes])\n",
        "        actions     = np.array([self.memory[index]['action'] for index in batch_indexes])\n",
        "        dones   = np.array([self.memory[index]['done'] for index in batch_indexes])\n",
        "        return {'states': states, 'next_states': next_states, 'rewards': rewards, 'actions': actions, 'dones': dones}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLFS72898L6B"
      },
      "source": [
        "class DqnAgent:\n",
        "    def __init__(self, num_state, num_action, gamma=0.99, lr=0.001, batch_size=32, memory_size=50000):\n",
        "        self.num_state = num_state\n",
        "        self.num_action = num_action\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.batch_size = batch_size  # Q関数の更新に用いる遷移の数\n",
        "        self.qnet = QNetwork(num_state, num_action)\n",
        "        self.target_qnet = copy.deepcopy(self.qnet)  # ターゲットネットワーク\n",
        "        self.optimizer = optim.Adam(self.qnet.parameters(), lr=lr)\n",
        "        self.replay_buffer = ReplayBuffer(memory_size)\n",
        "    \n",
        "    # Q関数を更新\n",
        "    def update_q(self):\n",
        "        batch = self.replay_buffer.sample(self.batch_size)\n",
        "        q = self.qnet(torch.tensor(batch[\"states\"], dtype=torch.float))\n",
        "        targetq = copy.deepcopy(q.data.numpy())\n",
        "        # maxQの計算\n",
        "        maxq = torch.max(self.target_qnet(torch.tensor(batch[\"next_states\"],dtype=torch.float)), dim=1).values\n",
        "        # targetqのなかで，バッチのなかで実際に選択されていた行動 batch[\"actions\"][i] に対応する要素に対して，Q値のターゲットを計算してセット\n",
        "        # 注意：選択されていない行動のtargetqの値はqと等しいためlossを計算する場合には影響しない\n",
        "        for i in range(self.batch_size):\n",
        "            # 終端状態の場合はmaxQを0にしておくと学習が安定します（ヒント：maxq[i] * (not batch[\"dones\"][i])）\n",
        "            targetq[i, batch[\"actions\"][i]] = batch[\"rewards\"][i] + self.gamma * maxq[i] * (not batch[\"dones\"][i]) \n",
        "        self.optimizer.zero_grad()\n",
        "        # lossとしてMSEを利用\n",
        "        loss = nn.MSELoss()(q, torch.tensor(targetq))\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # ターゲットネットワークのパラメータを更新\n",
        "        self.target_qnet = copy.deepcopy(self.qnet)\n",
        "    \n",
        "    # Q値が最大の行動を選択\n",
        "    def get_greedy_action(self, state):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float).view(-1, self.num_state)\n",
        "        action = torch.argmax(self.qnet(state_tensor).data).item()\n",
        "        return action\n",
        "    \n",
        "    # ε-greedyに行動を選択\n",
        "    def get_action(self, state, episode):\n",
        "        epsilon = 0.7 * (1/(episode+1))  # ここでは0.5から減衰していくようなεを設定\n",
        "        if epsilon <= np.random.uniform(0,1):\n",
        "            action = self.get_greedy_action(state)\n",
        "        else:\n",
        "            action = np.random.choice(self.num_action)\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwz5ukAR8L6B"
      },
      "source": [
        "# 各種設定\n",
        "num_episode = 300  # 学習エピソード数\n",
        "memory_size = 50000  # replay bufferの大きさ\n",
        "initial_memory_size = 500  # 最初に貯めるランダムな遷移の数\n",
        "\n",
        "# ログ\n",
        "episode_rewards = []\n",
        "num_average_epidodes = 10\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
        "\n",
        "agent = DqnAgent(env.observation_space.shape[0], env.action_space.n, memory_size=memory_size)\n",
        "\n",
        "# 最初にreplay bufferにランダムな行動をしたときのデータを入れる\n",
        "state = env.reset()\n",
        "for step in range(initial_memory_size):\n",
        "    action = env.action_space.sample() # ランダムに行動を選択        \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    transition = {\n",
        "        'state': state,\n",
        "        'next_state': next_state,\n",
        "        'reward': reward,\n",
        "        'action': action,\n",
        "        'done': int(done)\n",
        "    }\n",
        "    agent.replay_buffer.append(transition)\n",
        "    state = env.reset() if done else next_state\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    state = env.reset()  # envからは4次元の連続値の観測が返ってくる\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        action = agent.get_action(state, episode)  # 行動を選択\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        transition = {\n",
        "            'state': state,\n",
        "            'next_state': next_state,\n",
        "            'reward': reward,\n",
        "            'action': action,\n",
        "            'done': int(done)\n",
        "        }\n",
        "        agent.replay_buffer.append(transition)\n",
        "        agent.update_q()  # Q関数を更新\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    episode_rewards.append(episode_reward)\n",
        "    if episode % 20 == 0:\n",
        "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "\n",
        "# 累積報酬の移動平均を表示\n",
        "moving_average = np.convolve(episode_rewards, np.ones(num_average_epidodes)/num_average_epidodes, mode='valid')\n",
        "plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "plt.title('DQN: average rewards in %d episodes' % num_average_epidodes)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6GKVhjj8L6C"
      },
      "source": [
        "# 最終的に得られた方策のテスト（可視化）\n",
        "env = gym.make('CartPole-v0')\n",
        "frames = []\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_greedy_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "env.close()\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyPAOGjV8L6C"
      },
      "source": [
        "**やってみよう！（余力のある人向けの追加課題）**\n",
        "1. ハイパーパラメータを変化させた場合，学習曲線がどのように変化するかを観察してみましょう．\n",
        "  - とくに，価値関数の近似器として用いたニューラルネットワーク（`QNetwork`）の構造（層の数や中間層のユニットの数）を変更するとどうなるでしょうか．\n",
        "  - また，リプレイバッファの大きさ（`memory_size`）を変えるとどうなるでしょうか．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chFSVl1j8L6C"
      },
      "source": [
        "# LET'S TRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCYnoC3L8L6C"
      },
      "source": [
        "## 3.方策に基づく手法 (Policy-based Methods) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHo99dLu8L6D"
      },
      "source": [
        "### 3.1 方策勾配法：方策の近似\n",
        "- 方策勾配法では，方策を直接ニューラルネットワークで関数近似します．\n",
        "  - 実際には，方策と価値関数の両方を関数近似するactor-criticの手法（3.2項）を用いることも多いです．\n",
        "- 方策$\\pi_{\\theta}(\\cdot)$に対して目的関数$$\\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E_{\\pi_{\\theta}}}\\left[f_{\\pi_{\\theta}}(\\cdot)\\right]$$を最**大**化する$\\theta$を探索する手法です．\n",
        "  - $f_{\\pi_{\\theta}}(\\cdot)$：方策の良さを測る指標（詳しくは[Schulman 2015](https://arxiv.org/abs/1506.02438)参照）\n",
        "    - **REINFORCE**：収益（割引報酬和）$R_t=\\sum_{k=t}^{T} \\gamma^{k-t}r_{k+1}$を用いる場合\n",
        "    - **actor-critic**：状態価値や行動価値（の推定値）を用いる場合\n",
        "  - 実装上は目的関数の最小化を考えることが多いので，符号が反転します．\n",
        "- このとき**方策勾配**（policy gradient）は，\n",
        "$$\\nabla_{\\theta}\\mathcal{J}(\\pi_{\\theta}) = \\mathbb{E_{\\pi_{\\theta}}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta} \\cdot f_{\\pi_{\\theta}}(\\cdot)\\right]$$と定義されます．"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOI_v_eq8L6D"
      },
      "source": [
        "#### 3.1.1 REINFORCE\n",
        "- REINFORCEは，方策の良さの指標$f_{\\pi_{\\theta}}(\\cdot)$として収益$$\\begin{align}R_t &= r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\cdots +\\gamma^{T-t-1} r_T \\\\ &= \\sum_{k=t}^{T} \\gamma^{k-t}r_{k+1}\\end{align}$$を用いる手法です．つまり，$$f_{\\pi_{\\theta}}(\\cdot)=R_t$$とすることで，方策勾配は，\n",
        "$$\\nabla_{\\theta}\\mathcal{J}(\\pi_{\\theta}) = \\frac{1}{T}\\sum_{t=1}^{T}\\mathbb{E_{\\pi_{\\theta}}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\cdot R_t\\right]$$となります．\n",
        "- REINFORCEでは，収益$R_t$の値は分散は一般に大きいため，方策勾配の分散が大きくなり，学習が不安定になりやすいことが知られています．\n",
        "  - そのため，収益$R_t$から**ベースライン**(baseline)$b(s)$を引く，つまり，$$f_{\\pi_{\\theta}}(\\cdot)=R_t-b(s)$$とすることで，学習の安定化を図ることがよくあります．\n",
        "    - ベースライン$b(s)$を引いても，方策勾配の期待値には影響しないことが知られています．\n",
        "- REINFORCEでは，確率的な方策を採用しています．\n",
        "  - 今回のような離散の行動空間では，NNがそれぞれの行動の選択確率を出力するように実装することがよくあります．\n",
        "    - 確率的な行動の選択は[カテゴリカル分布](https://pytorch.org/docs/stable/distributions.html#categorical)$P(x=k ; \\mathbf{p})=p_{k}$からサンプリングを行う実装が多いです．\n",
        "    - テスト時には，出力された確率値が最大の行動を選びます（greedy方策）．\n",
        "\n",
        "**論文**\n",
        "- [Simple statistical gradient-following algorithms for connectionist reinforcement learning](https://link.springer.com/article/10.1007/BF00992696)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Paa8ls9j8L6D"
      },
      "source": [
        "# 方策のネットワークの定義\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, num_state, num_action, hidden_size=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_state, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, num_action)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = F.elu(self.fc1(x))\n",
        "        h = F.elu(self.fc2(h))\n",
        "        action_prob = F.softmax(self.fc3(h), dim=-1)\n",
        "        return action_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6V3y6tm8L6D"
      },
      "source": [
        "class ReinforceAgent:\n",
        "    def __init__(self, num_state, num_action, gamma=0.99, lr=0.001):\n",
        "        self.num_state = num_state\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.pinet = PolicyNetwork(num_state, num_action)\n",
        "        self.optimizer = optim.Adam(self.pinet.parameters(), lr=lr)\n",
        "        self.memory = []  # 報酬とそのときの行動選択確率のtupleをlistで保存\n",
        "    \n",
        "    # 方策を更新\n",
        "    def update_policy(self):\n",
        "        R = 0\n",
        "        loss = 0\n",
        "        # エピソード内の各ステップの収益を後ろから計算\n",
        "        for r, prob in self.memory[::-1]:\n",
        "            R = r + self.gamma * R\n",
        "            loss -= torch.log(prob) * R\n",
        "        loss = loss/len(self.memory)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "    # softmaxの出力が最も大きい行動を選択\n",
        "    def get_greedy_action(self, state):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float).view(-1, self.num_state)\n",
        "        action_prob = self.pinet(state_tensor.data).squeeze()\n",
        "        action = torch.argmax(action_prob.data).item()\n",
        "        return action\n",
        "    \n",
        "    # カテゴリカル分布からサンプリングして行動を選択\n",
        "    def get_action(self, state):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float).view(-1, self.num_state)\n",
        "        action_prob = self.pinet(state_tensor.data).squeeze()\n",
        "        action = Categorical(action_prob).sample().item()\n",
        "        return action, action_prob[action]\n",
        "    \n",
        "    def add_memory(self, r, prob):\n",
        "        self.memory.append((r, prob))\n",
        "    \n",
        "    def reset_memory(self):\n",
        "        self.memory = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hawme2fK8L6D"
      },
      "source": [
        "# 各種設定\n",
        "num_episode = 600  # 学習エピソード数\n",
        "# penalty = 10  # 途中でエピソードが終了したときのペナルティ\n",
        "\n",
        "# ログ\n",
        "episode_rewards = []\n",
        "num_average_epidodes = 10\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
        "\n",
        "agent = ReinforceAgent(env.observation_space.shape[0], env.action_space.n)\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    state = env.reset()  # envからは4次元の連続値の観測が返ってくる\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        action, prob = agent.get_action(state)  #  行動を選択\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "#         # もしエピソードの途中で終了してしまったらペナルティを加える\n",
        "#         if done and t < max_steps - 1:\n",
        "#             reward = - penalty\n",
        "        episode_reward += reward\n",
        "        agent.add_memory(reward, prob)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            agent.update_policy()\n",
        "            agent.reset_memory()\n",
        "            break\n",
        "    episode_rewards.append(episode_reward)\n",
        "    if episode % 20 == 0:\n",
        "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "\n",
        "# 累積報酬の移動平均を表示\n",
        "moving_average = np.convolve(episode_rewards, np.ones(num_average_epidodes)/num_average_epidodes, mode='valid')\n",
        "plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "plt.title('REINFORCE: average rewards in %d episodes' % num_average_epidodes)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Wr8hvk8L6E"
      },
      "source": [
        "# 最終的に得られた方策のテスト（可視化）\n",
        "env = gym.make('CartPole-v0')\n",
        "frames = []\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = agent.get_greedy_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "env.close()\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH2wYBde8L6F"
      },
      "source": [
        "**やってみよう！（余力のある人向けの追加課題）**\n",
        "1. ハイパーパラメータ・ネットワークを変更した場合，学習曲線がどのように変化するかを観察してみましょう．\n",
        "2. 報酬から適当なベースラインを引いて実験してみましょう．\n",
        "  - 現在までの経験の平均報酬をベースラインとして利用する方法が一般的です．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcbuLJSf8L6F"
      },
      "source": [
        "# LET'S TRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03e7XLuC8L6F"
      },
      "source": [
        "### 3.2 Actor-Critic：方策と価値関数の近似"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEyau_uJ8L6G"
      },
      "source": [
        "#### 3.2.1 Actor-Criticの実装例\n",
        "- 方策（actor）と価値関数（critic）の両方をニューラルネットワークで近似します．\n",
        "  - 今回の実装では，状態価値関数をモデル化して，$$f_{\\pi_{\\theta}}(\\cdot)=R_t-V^{\\pi_{\\theta}}(s)$$として実装してみます．\n",
        "    - この実装は，REINFORCEにおけるベースライン$b(s)$を$V^{\\pi_{\\theta}}(s)$とした場合とも解釈できます．\n",
        "  \n",
        "**補足説明**\n",
        "- とくに，$$f_{\\pi_{\\theta}}(\\cdot) = Q^{\\pi_{\\theta}}(s, a) - V^{\\pi_{\\theta}}(s)$$とした場合の$f_{\\pi_{\\theta}}(\\cdot)$は**アドバンテージ**（advantage）$A^{\\pi_{\\theta}}(s,a)$と呼ばれています．\n",
        "  - これは，状態$s$における行動$a$の相対的な良さを表しています．\n",
        "  - この組み合わせ以外の場合にも，[一般化アドバンテージ（generalized advantage estimation, GAE）](https://arxiv.org/abs/1506.02438)として単に「アドバンテージ」ということがあります．\n",
        "- 実装上，actorとcriticのネットワークの共有はよく行われます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BgsClt8L6G"
      },
      "source": [
        "# actorとcriticのネットワーク（一部の重みを共有しています）\n",
        "class ActorCriticNetwork(nn.Module):\n",
        "    def __init__(self, num_state, num_action, hidden_size=16):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_state, hidden_size)\n",
        "        self.fc2a = nn.Linear(hidden_size, num_action)  # actor独自のlayer\n",
        "        self.fc2c = nn.Linear(hidden_size, 1)  # critic独自のlayer\n",
        "    \n",
        "    def forward(self, x):\n",
        "        h = F.elu(self.fc1(x))\n",
        "        action_prob = F.softmax(self.fc2a(h), dim=-1)\n",
        "        state_value = self.fc2c(h)\n",
        "        return action_prob, state_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaBLic3A8L6G"
      },
      "source": [
        "class ActorCriticAgent:\n",
        "    def __init__(self, num_state, num_action, gamma=0.99, lr=0.001):\n",
        "        self.num_state = num_state\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.acnet = ActorCriticNetwork(num_state, num_action)\n",
        "        self.optimizer = optim.Adam(self.acnet.parameters(), lr=lr)\n",
        "        self.memory = []  # （報酬，行動選択確率，状態価値）のtupleをlistで保存\n",
        "        \n",
        "    # 方策を更新\n",
        "    def update_policy(self):\n",
        "        R = 0\n",
        "        actor_loss = 0\n",
        "        critic_loss = 0\n",
        "        for r, prob, v in self.memory[::-1]:\n",
        "            R = r + self.gamma * R\n",
        "            advantage = R - v\n",
        "            actor_loss -= torch.log(prob) * advantage\n",
        "            critic_loss += F.smooth_l1_loss(v, torch.tensor(R))\n",
        "        actor_loss = actor_loss/len(self.memory)\n",
        "        critic_loss = critic_loss/len(self.memory)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss = actor_loss + critic_loss\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    \n",
        "    # softmaxの出力が最も大きい行動を選択\n",
        "    def get_greedy_action(self, state):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float).view(-1, self.num_state)\n",
        "        action_prob, _ = self.acnet(state_tensor.data)\n",
        "        action = torch.argmax(action_prob.squeeze().data).item()\n",
        "        return action\n",
        "        \n",
        "    # カテゴリカル分布からサンプリングして行動を選択\n",
        "    def get_action(self, state):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float).view(-1, self.num_state)\n",
        "        action_prob, state_value = self.acnet(state_tensor.data)\n",
        "        action_prob, state_value = action_prob.squeeze(), state_value.squeeze()\n",
        "        action = Categorical(action_prob).sample().item()\n",
        "        return action, action_prob[action], state_value\n",
        "    \n",
        "    def add_memory(self, r, prob, v):\n",
        "        self.memory.append((r, prob, v))\n",
        "    \n",
        "    def reset_memory(self):\n",
        "        self.memory = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwPcDx6k8L6H"
      },
      "source": [
        "# 各種設定\n",
        "num_episode = 1200  # 学習エピソード数\n",
        "# penalty = 10  # 途中でエピソードが終了したときのペナルティ\n",
        "\n",
        "# ログ\n",
        "episode_rewards = []\n",
        "num_average_epidodes = 10\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
        "\n",
        "agent = ActorCriticAgent(env.observation_space.shape[0], env.action_space.n)\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    state = env.reset()  # envからは4次元の連続値の観測が返ってくる\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        action, prob, state_value = agent.get_action(state)  #  行動を選択\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "#         # もしエピソードの途中で終了してしまったらペナルティを加える\n",
        "#         if done and t < max_steps - 1:\n",
        "#             reward = - penalty\n",
        "        episode_reward += reward\n",
        "        agent.add_memory(reward, prob, state_value)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            agent.update_policy()\n",
        "            agent.reset_memory()\n",
        "            break\n",
        "    episode_rewards.append(episode_reward)\n",
        "    if episode % 50 == 0:\n",
        "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "\n",
        "# 累積報酬の移動平均を表示\n",
        "moving_average = np.convolve(episode_rewards, np.ones(num_average_epidodes)/num_average_epidodes, mode='valid')\n",
        "plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "plt.title('Actor-Critic: average rewards in %d episodes' % num_average_epidodes)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuINiCzi8L6H"
      },
      "source": [
        "# 最終的に得られた方策のテスト（可視化）\n",
        "env = gym.make('CartPole-v0')\n",
        "frames = []\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    terminal = False\n",
        "    while not terminal:\n",
        "        action = agent.get_greedy_action(state)\n",
        "        state, reward, terminal, _ = env.step(action)\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "env.close()\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6RoU-Nk8L6H"
      },
      "source": [
        "**やってみよう！（余力のある人向けの追加課題）**\n",
        "1. ハイパーパラメータ・ネットワークを変更した場合，学習曲線がどのように変化するかを観察してみましょう．\n",
        "  - とくに，criticのネットワークを変更するとどうなるでしょうか．\n",
        "1. Q値（行動価値）も推定して，アドバンテージを利用したactor-criticアルゴリズムを実装してみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIfldNs38L6I"
      },
      "source": [
        "# LET'S TRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFSGItOC8L6I"
      },
      "source": [
        "### 3.3 連続値行動空間\n",
        "最後に，ニューラルネットワークによって価値関数や方策の関数近似をしたactor-critic手法を，連続値の行動空間を持つ問題に利用してみましょう．\n",
        "- 古典的には，REINFORCEで出力層に仮定した分布をカテゴリカル分布から正規分布（ガウス分布）に変更することで，連続値を出力する実装がよく利用されてきました．\n",
        "- 近年では，[DDPG](https://arxiv.org/abs/1509.02971)（Deep Deterministic Policy Gradient）や，[SAC](https://arxiv.org/abs/1801.01290)（Soft Actor-Critc）と呼ばれる手法が用いられることが多いです．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE-N9Iu88L6I"
      },
      "source": [
        "#### 3.3.1 連続値行動空間の環境の例：pendulum-v0\n",
        "- OpenAI Gymには，連続値行動空間の環境のシミュレータも含まれています．\n",
        "- ここでは`pendulum-v0`という，軸の角速度を制御して振子を立てる問題を解いてみましょう（[詳細](https://github.com/openai/gym/wiki/Pendulum-v0)）．\n",
        "    - 状態空間：3つの連続値\n",
        "        1. $\\cos(\\theta)$：振子の向き$\\theta$のコサイン $[-1.0,1.0]$\n",
        "        1. $\\sin(\\theta)$：振子の向き$\\theta$のサイン $[-1.0,1.0]$\n",
        "        1. $\\dot{\\theta}$：振子の角速度 $[-8.0,8.0]$\n",
        "    - 行動空間：1つの連続値．\n",
        "      - $a$：関節（中心）にかける力 $[-2.0,2.0]$\n",
        "    - エピソードの長さが200ステップに達したときにエピソードが終了します．\n",
        "    - 報酬：終端条件に達するまで常に$-(\\theta^2 + 0.1\\times\\dot{theta}^2 + 0.001\\times a^2)$\n",
        "    \n",
        "環境を作成してランダムな行動を取ってみましょう．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTu3zT9S8L6I"
      },
      "source": [
        "env = gym.make('Pendulum-v0')  # シミュレータ環境の構築\n",
        "frames = []\n",
        "for episode in range(3):\n",
        "    state = env.reset()  # エピソードを開始（環境の初期化）\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # 行動をランダムに選択\n",
        "        next_state, reward, done, _ = env.step(action)  # 行動を実行し、次の状態、 報酬、 終端か否かの情報を取得\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "env.close()  # 画面出力の終了\n",
        "\n",
        "# 結果の確認\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIKCH1S18L6I"
      },
      "source": [
        "#### 3.3.2 Deep Deterministic Policy Gradient (DDPG)\n",
        "\n",
        "DDPG（Deep Deterministic Policy Gradient）は，actor（方策）とcritic（行動価値関数）の両方をニューラルネットワークで近似した手法です．\n",
        "\n",
        "全体として，**連続値の行動空間に対してDQNを用いた手法**になっています．\n",
        "- 連続値の行動空間に対して$\\arg\\max_{a'}Q_{\\phi}^{\\pi}(s_{t},a')$を求めるのは難しいので，方策もNNでパラメータ化することで解決しています．\n",
        "- そのためにactor$\\pi_{\\theta}$に**決定論的な方策**を用いています．\n",
        "  - なお，方策$\\pi_{\\theta}$が決定論的な場合，$\\mu_{\\theta}$と明示的に表記する文献もあります．\n",
        "\n",
        "以上の設定によって，criticの学習はDQNと同様に，TD誤差$$\\delta_t = r_{t+1}+\\gamma Q_{\\phi}^{\\pi_{\\theta}}(s_{t+1},\\pi_{\\theta}(s_t)) - Q_{\\phi}^{\\pi_{\\theta}}(s_t,a_t)$$の最**小**化をすれば良いことがわかります．\n",
        "\n",
        "一方，actorはQ関数$Q_{\\phi}\\left(s, \\pi_{\\theta}(s)\\right)$の値が最**大**になるように方策勾配法を用いて学習します．つまり，actorの方策勾配は$$\\nabla_{\\theta}\\mathcal{J}(\\pi_{\\theta})=\\nabla_{\\theta} Q_{\\phi}\\left(s, \\pi_{\\theta}(s)\\right)$$となります．\n",
        "\n",
        "実装では，これらのcriticとactorの更新を交互に繰り返して最適化を行います．\n",
        "\n",
        "\n",
        "**論文**\n",
        "- [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g50aNKnp8L6I"
      },
      "source": [
        "# actorのネットワーク\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, num_state, action_space, hidden_size=16):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.action_mean = torch.tensor(0.5*(action_space.high+action_space.low), dtype=torch.float)\n",
        "        self.action_halfwidth = torch.tensor(0.5*(action_space.high-action_space.low), dtype=torch.float)\n",
        "        self.fc1 = nn.Linear(num_state, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_space.shape[0])\n",
        "\n",
        "    def forward(self, s):\n",
        "        h = F.elu(self.fc1(s))\n",
        "        h = F.elu(self.fc2(h))\n",
        "        a = self.action_mean + self.action_halfwidth*torch.tanh(self.fc3(h))\n",
        "        return a\n",
        "\n",
        "# criticのネットワーク（状態と行動を入力にしてQ値を出力）\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, num_state, action_space, hidden_size=16):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.action_mean = torch.tensor(0.5*(action_space.high+action_space.low), dtype=torch.float)\n",
        "        self.action_halfwidth = torch.tensor(0.5*(action_space.high-action_space.low), dtype=torch.float)\n",
        "        self.fc1 = nn.Linear(num_state+action_space.shape[0], hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, action_space.shape[0])\n",
        "\n",
        "    def forward(self, s, a):\n",
        "        a = (a-self.action_mean)/self.action_halfwidth\n",
        "        h = F.elu(self.fc1(torch.cat([s,a],1)))\n",
        "        h = F.elu(self.fc2(h))\n",
        "        q = self.fc3(h)\n",
        "        return q\n",
        "\n",
        "# リプレイバッファの定義\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, memory_size):\n",
        "        self.memory_size = memory_size\n",
        "        self.memory = deque([], maxlen = memory_size)\n",
        "    \n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size)\n",
        "        states      = np.array([self.memory[index]['state'] for index in batch_indexes])\n",
        "        next_states = np.array([self.memory[index]['next_state'] for index in batch_indexes])\n",
        "        rewards     = np.array([self.memory[index]['reward'] for index in batch_indexes])\n",
        "        actions     = np.array([self.memory[index]['action'] for index in batch_indexes])\n",
        "        dones   = np.array([self.memory[index]['done'] for index in batch_indexes])\n",
        "        return {'states': states, 'next_states': next_states, 'rewards': rewards, 'actions': actions, 'dones': dones}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW6zRaCN8L6I"
      },
      "source": [
        "class DdpgAgent:\n",
        "    def __init__(self, observation_space, action_space, gamma=0.99, lr=1e-3, batch_size=32, memory_size=50000):\n",
        "        self.num_state = observation_space.shape[0]\n",
        "        self.num_action = action_space.shape[0]\n",
        "        self.state_mean = 0.5*(observation_space.high + observation_space.low)\n",
        "        self.state_halfwidth = 0.5*(observation_space.high - observation_space.low)\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.batch_size = batch_size\n",
        "        self.actor = ActorNetwork(self.num_state, action_space)\n",
        "        self.actor_target = copy.deepcopy(self.actor)  # actorのターゲットネットワーク\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
        "        self.critic = CriticNetwork(self.num_state, action_space)\n",
        "        self.critic_target = copy.deepcopy(self.critic)  # criticのターゲットネットワーク\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
        "        self.replay_buffer = ReplayBuffer(memory_size)\n",
        "    \n",
        "    # 連続値の状態を[-1,1]の範囲に正規化\n",
        "    def normalize_state(self, state):\n",
        "        state = (state-self.state_mean)/self.state_halfwidth\n",
        "        return state\n",
        "    \n",
        "    # リプレイバッファからサンプルされたミニバッチをtensorに変換\n",
        "    def batch_to_tensor(self, batch):\n",
        "        states = torch.tensor([self.normalize_state(s) for s in batch[\"states\"]], dtype=torch.float)\n",
        "        actions = torch.tensor(batch[\"actions\"], dtype=torch.float)\n",
        "        next_states = torch.tensor([self.normalize_state(s) for s in batch[\"next_states\"]], dtype=torch.float)\n",
        "        rewards = torch.tensor(batch[\"rewards\"], dtype=torch.float)\n",
        "        return states, actions, next_states, rewards\n",
        "    \n",
        "    # actorとcriticを更新\n",
        "    def update(self):\n",
        "        batch = self.replay_buffer.sample(self.batch_size)\n",
        "        states, actions, next_states, rewards = self.batch_to_tensor(batch)\n",
        "        # criticの更新\n",
        "        target_q = (rewards + self.gamma*self.critic_target(next_states, self.actor_target(next_states)).squeeze()).data\n",
        "        q = self.critic(states, actions).squeeze()\n",
        "        critic_loss = F.mse_loss(q, target_q)\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        # actorの更新\n",
        "        actor_loss = -self.critic(states, self.actor(states)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        # ターゲットネットワークのパラメータを更新\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "    \n",
        "    # Q値が最大の行動を選択\n",
        "    def get_action(self, state):\n",
        "        state_tensor = torch.tensor(self.normalize_state(state), dtype=torch.float).view(-1, self.num_state)\n",
        "        action = self.actor(state_tensor).view(self.num_action)\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TnzqS868L6I"
      },
      "source": [
        "# 各種設定\n",
        "num_episode = 250  # 学習エピソード数（学習に時間がかかるので短めにしています）\n",
        "memory_size = 50000  # replay bufferの大きさ\n",
        "initial_memory_size = 1000  # 最初に貯めるランダムな遷移の数\n",
        "# ログ用の設定\n",
        "episode_rewards = []\n",
        "num_average_epidodes = 10\n",
        "\n",
        "env = gym.make('Pendulum-v0')\n",
        "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
        "agent = DdpgAgent(env.observation_space, env.action_space, memory_size=memory_size)\n",
        "\n",
        "# 最初にreplay bufferにランダムな行動をしたときのデータを入れる\n",
        "state = env.reset()\n",
        "for step in range(initial_memory_size):\n",
        "    action = env.action_space.sample() # ランダムに行動を選択 \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    transition = {\n",
        "        'state': state,\n",
        "        'next_state': next_state,\n",
        "        'reward': reward,\n",
        "        'action': action,\n",
        "        'done': int(done)\n",
        "    }\n",
        "    agent.replay_buffer.append(transition)\n",
        "    state = env.reset() if done else next_state\n",
        "print('%d Data collected' % (initial_memory_size))\n",
        "\n",
        "for episode in range(num_episode):\n",
        "    state = env.reset()  # envからは3次元の連続値の観測が返ってくる\n",
        "    episode_reward = 0\n",
        "    for t in range(max_steps):\n",
        "        action = agent.get_action(state).data.numpy()  #  行動を選択\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        transition = {\n",
        "            'state': state,\n",
        "            'next_state': next_state,\n",
        "            'reward': reward,\n",
        "            'action': action,\n",
        "            'done': int(done)\n",
        "        }\n",
        "        agent.replay_buffer.append(transition)\n",
        "        agent.update()  # actorとcriticを更新\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "    episode_rewards.append(episode_reward)\n",
        "    if episode % 20 == 0:\n",
        "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
        "\n",
        "# 累積報酬の移動平均を表示\n",
        "moving_average = np.convolve(episode_rewards, np.ones(num_average_epidodes)/num_average_epidodes, mode='valid')\n",
        "plt.plot(np.arange(len(moving_average)),moving_average)\n",
        "plt.title('DDPG: average rewards in %d episodes' % num_average_epidodes)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('rewards')\n",
        "plt.show()\n",
        "\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qwxcPdx8L6J"
      },
      "source": [
        "# 最終的に得られた方策のテスト（可視化）\n",
        "env = gym.make('Pendulum-v0')\n",
        "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
        "frames = []\n",
        "for episode in range(3):\n",
        "    state = env.reset()\n",
        "    frames.append(env.render(mode='rgb_array'))\n",
        "    for t in range(max_steps):\n",
        "        action = agent.get_action(state).detach().numpy()\n",
        "        state, reward, terminal, _ = env.step(action)\n",
        "        frames.append(env.render(mode='rgb_array'))\n",
        "env.close()\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "  \n",
        "def animate(i):\n",
        "    patch.set_data(frames[i])\n",
        "    \n",
        "anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPV2uZhs8L6J"
      },
      "source": [
        "**やってみよう！（余力のある人向けの追加課題）**\n",
        "1. ハイパーパラメータ・ネットワークを変更した場合，学習曲線がどのように変化するかを観察してみましょう．\n",
        "1. `pendulum-v0`以外の連続値行動空間のタスクを解いてみましょう．\n",
        "  - OpenAI Gymで利用できる環境の一覧は，[webサイト](https://gym.openai.com/envs/)や[Github wiki](https://github.com/openai/gym/wiki/Table-of-environments)に掲載されています．\n",
        "  - おすすめの環境\n",
        "    - `MountainCarContinuous-v0`：車を山の頂上に到達させるタスク．入力の次元も小さく（2次元）で比較的簡単．\n",
        "    - 以下の環境は，`box2d`と`gym[Box2D]`を追加でインストールする必要があります（notebook上では`!pip install box2d gym[Box2D]`で入ります）．\n",
        "      - `LunarLanderContinuous-v2`：宇宙船を指定された位置に着陸させるタスク．\n",
        "      - `BipedalWalker-v3`：2足歩行ロボットを歩かせるタスク．入力次元が24次元で少し難しい．\n",
        "      - `BipedalWalkerHardcore-v3`：上記環境をもっと難しくしたタスク（障害物がある）．\n",
        "1. 今回の講義で紹介されたもの以外の連続値行動空間の深層強化学習手法を調べて実装してみましょう．既存の手法の課題として何が挙げられていて，それに対してどのような工夫がなされているでしょうか．\n",
        "  - 近年の代表的な手法の例\n",
        "    - [TRPO（Trust Region Policy Optimization）](https://arxiv.org/abs/1502.05477)\n",
        "    - [PPO（Proximal Policy Optimization）](https://arxiv.org/abs/1707.06347)\n",
        "    - [SAC（Soft Actor-Critic）](https://arxiv.org/abs/1801.01290)\n",
        "    - [TD3（Twin-Delayed DDPG）](https://arxiv.org/abs/1802.09477)\n",
        "    - [AWR（Advantage-Weighted Regression）](https://arxiv.org/abs/1910.00177)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hff9vstc8L6J"
      },
      "source": [
        "# LET'S TRY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ1x53tq8L6J"
      },
      "source": [
        "**補足説明**\n",
        "- 今回の演習では状態としてロボットの関節角や角速度などを用いる問題設定を考えましたが，CNNなどを用いて，画像を入力とした連続値制御をすることもできます．"
      ]
    }
  ]
}