{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"chap06_exercise_answer.ipynb","provenance":[],"collapsed_sections":["WoEBGDQZ86LV","xzvyahYC86Lh","9S_oclNi86Ll","0tOQLs6m86Lq","Jpi31tv286L4","JTrJiFvh86L-","khVKkNVw86MI","a-S4R09386Mb","bTDoX5hV86Mg","ApMPRmr186Mm"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ksd5ro1y86Ko"},"source":["# 第4回講義 演習"]},{"cell_type":"markdown","metadata":{"id":"mM0KhaWJ86Ks"},"source":["## 目次\n","\n","- 課題. 畳み込みニューラルネットワーク(Convolutional Neural Networks)の実装と学習\n","    1. 畳み込みとプーリング in pytorch\n","        - 1.1. 畳み込み: torch.nn.functional.conv2d\n","        - 1.2. プーリング: torch.nn.functional.max_pool2d, torch.nn.functional.avg_pool2d, etc.\n","    2. 各層クラスの実装\n","        - 2.1. 畳み込み層\n","        - 2.2. プーリング層\n","        - 2.3.平滑化層（4次元->2次元）\n","        - 2.4. 全結合層\n","    3. MNISTを用いたクラス分類\n","    4. 学習"]},{"cell_type":"markdown","metadata":{"id":"yzZT7fsO86Kt"},"source":["## 課題. 畳み込みニューラルネットワーク(Convolutional Neural Networks)の実装と学習"]},{"cell_type":"code","metadata":{"id":"xHrmYBb286Ky"},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms, models\n","from sklearn.utils import shuffle\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","rng = np.random.RandomState(1234)\n","random_state = 42"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yYxpVUVSDkIM"},"source":["## 0. nn.functional"]},{"cell_type":"markdown","metadata":{"id":"H8Jn0bb4Dwtn"},"source":["前回は実装理解の為用いませんでしたが、\n","`nn.functional`には、一般的な演算に用いられる関数が一通り用意されています。\n","\n","これを用いることで、一つ一つ実装すると手間のかかる演算も簡単に使うことができるようになります。"]},{"cell_type":"code","metadata":{"id":"jd1nKCOMDwUL"},"source":["torch.manual_seed(34)\n","\n","a = torch.arange(6, dtype=torch.float32).reshape(2, 3) - 3\n","print(a)\n","print()\n","\n","# ReLU関数\n","b = F.relu(a)\n","print(\"ReLU関数\")\n","print(b)\n","print()\n","\n","# シグモイド関数\"F.sigmoid\"も用意されているがdepricatedであり、torch.sigmoidが推奨\n","b = torch.sigmoid(a)\n","print(\"シグモイド関数\")\n","print(b)\n","print()\n","\n","# ソフトマックス関数\n","b = F.softmax(a, dim=-1)  # 正規化したいdimを指定する. 最後のdimに対して行いたいときはdim=-1.\n","print(\"ソフトマックス関数\")\n","print(b)\n","print()\n","\n","\n","a = np.array([[1, 1, 1, 0, 0],\n","              [0, 1, 1, 1, 0],\n","              [0, 0, 1, 1, 1],\n","              [0, 0, 1, 1, 0],\n","              [0, 1, 1, 0, 0]]\n","             ).astype('float32').reshape(1, 1, 5, 5)\n","a = torch.tensor(a)\n","\n","# パラメータ\n","W = np.array([[1, 0, 1],\n","              [0, 1, 0],\n","              [1, 0, 1]]).astype('float32').reshape(1, 1, 3, 3)\n","W = torch.tensor(W)\n","\n","b = F.conv2d(a, W, bias=None, stride=(1, 1), padding=0)\n","\n","print(\"畳み込み演算\")\n","print(b)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PHemZkLE86K_"},"source":["## 1. 畳み込みとプーリング in pytorch"]},{"cell_type":"markdown","metadata":{"id":"QUgVQjYp86LB"},"source":["### 1.1. 畳み込み: torch.nn.functional.conv2d [\\[link\\]](https://pytorch.org/docs/stable/nn.functional.html#conv2d)"]},{"cell_type":"markdown","metadata":{"id":"oB7zzqxj86LC"},"source":["- 入力または隠れ層$X_{i, j}^{k}$\n","    - 次元数4$(n,k,i,j)$\n","        - $n$：バッチサイズ\n","        - $k$：入力のチャネル数\n","        - $i$：入力の行数\n","        - $j$：入力の列数\n","- 畳み込みのフィルタ（重み）$W_{i,j}^{k,l}$\n","    - 次元数4$(l,k,i,j)$\n","        - $l$: 出力のチャネル数(フィルタ数)\n","        - $k$: 入力のチャネル数\n","        - $i$: フィルタの行数\n","        - $j$: フィルタの列数\n","    - ストライド：フィルタを適用する位置の間隔\n","    - ゼロパディング：入力の周りに値0の縁を加えます\n","        - 入力のサイズを保つ為、フィルタの縦or横の次元が$F$のときパディング数を$(F-1)/2$とします。\n","- フィルタ後のサイズは、入力の縦or横の次元数$N$、フィルタの縦or横の次元数$F$、ストライドの縦or横の量$S$で決まります。\n","    - $ceil((N-F+1)/S)$ (ceilは整数値に切り上げ)"]},{"cell_type":"markdown","metadata":{"id":"I9P0UpW586LC"},"source":["### 畳み込みの適用例"]},{"cell_type":"code","metadata":{"id":"54y6AyGM86LD"},"source":["# サンプル画像\n","sample_image = np.array([[1, 1, 1, 0, 0],\n","                         [0, 1, 1, 1, 0],\n","                         [0, 0, 1, 1, 1],\n","                         [0, 0, 1, 1, 0],\n","                         [0, 1, 1, 0, 0]]\n","                        ).astype('float32').reshape(1, 1, 5, 5)  # バッチサイズ x チャンネル数 x 高さ x 幅\n","sample_image = torch.tensor(sample_image)\n","\n","# フィルタ\n","W = np.array([[1, 0, 1],\n","              [0, 1, 0],\n","              [1, 0, 1]]).astype('float32').reshape(1, 1, 3, 3)  # 出力チャンネル数 x 入力チャンネル数 x 高さ x 幅 　\n","W = torch.tensor(W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElV1Rt6x86LH"},"source":["#### 畳み込み1\n","\n","- バイアス: なし\n","- ストライド: (1, 1)\n","- パディング: なし\n","- 出力のサイズ: ceil((5-3+1)/1)=3"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"7JYW-cFA86LJ"},"source":["convoluted_image = F.conv2d(sample_image, W, bias=None, stride=(1, 1), padding=0)\n","\n","print(convoluted_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXztSUSN86LQ"},"source":["#### 畳み込み2\n","\n","- バイアス: なし\n","- ストライド: (2, 2)\n","- パディング: なし\n","- 出力のサイズ: ceil((5-3+1)/2)=2"]},{"cell_type":"code","metadata":{"id":"hiJLwyz786LR"},"source":["convoluted_image = F.conv2d(sample_image, W, bias=None, stride=(2, 2), padding=0)\n","\n","print(convoluted_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WoEBGDQZ86LV"},"source":["#### 畳み込み3\n","- ストライド: (1, 1)\n","- パディング: 上下左右に1ずつ\n","\n","パディングの大きさをxとすると\n","出力のサイズ: ceil((5+(2*x)-3+1)/1)=5\n","である。（入力サイズと出力サイズが等しくなるように調整した）"]},{"cell_type":"code","metadata":{"id":"aEakdvng86LX"},"source":["convoluted_image = F.conv2d(sample_image, W, bias=None, stride=(1, 1), padding=1)\n","\n","print(convoluted_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vs9_h5fk86Lb"},"source":["### 畳み込みの演習"]},{"cell_type":"code","metadata":{"id":"fMXPe1TG86Lc"},"source":["# サンプル画像\n","sample_image = np.array([[1, 1, 1, 0, 0, 1, 0],\n","                         [0, 1, 0, 1, 0, 1, 1],\n","                         [1, 0, 1, 1, 1, 0, 1],\n","                         [0, 0, 1, 1, 0, 1, 1],\n","                         [1, 1, 1, 1, 0, 0, 1],\n","                         [0, 1, 1, 1, 1, 1, 1],\n","                         [0, 1, 1, 0, 0, 1, 0]]\n","                        ).astype('float32').reshape(1, 1, 7, 7)\n","sample_image = torch.tensor(sample_image)\n","\n","# フィルタ\n","W = np.array([[1, 0, 1],\n","              [0, 1, 0],\n","              [1, 0, 1]]).astype('float32').reshape(1, 1, 3, 3)\n","W = torch.tensor(W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzvyahYC86Lh"},"source":["#### 畳み込み1\n","\n","- ストライド: (1, 1)\n","- パディング: なし ('VALID')"]},{"cell_type":"code","metadata":{"id":"PF2bMl9286Li"},"source":["convoluted_image = F.conv2d(sample_image, W, bias=None, stride=(1, 1), padding=0)\n","\n","print(convoluted_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9S_oclNi86Ll"},"source":["#### 畳み込み2\n","\n","- ストライド: (2, 2)\n","- パディング: なし"]},{"cell_type":"code","metadata":{"id":"ZWvrdrKN86Lm"},"source":["convoluted_image = F.conv2d(sample_image, W, bias=None, stride=(2, 2), padding=0)\n","\n","print(convoluted_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tOQLs6m86Lq"},"source":["#### 畳み込み3\n","\n","- ストライド: (1, 1)\n","- パディング: 入力サイズと等しい(paddingを計算する必要あり)"]},{"cell_type":"code","metadata":{"id":"JD1ow3NI86Lr"},"source":["convoluted_image = F.conv2d(sample_image, W, bias=None, stride=(1, 1), padding=1)\n","\n","print(convoluted_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GRUbufAW86Lv"},"source":["### 1.2. プーリング: torch.nn.functional.max_pool2d \\[[link\\]](https://pytorch.org/docs/stable/nn.functional.html#max-pool2d), torch.nn.functional.avg_pool2d \\[[link\\]](https://pytorch.org/docs/stable/nn.functional.html#avg-pool2d), etc."]},{"cell_type":"markdown","metadata":{"id":"Z7lNiEL586Lx"},"source":["- プーリングには次の種類があります。\n","    - Max pooling\n","    - Sum pooling\n","    - Mean pooling\n","    - その他Lpプーリングなど\n","- 畳み込みと同様、ストライドやパディングも考えることがあります。\n","- プーリング後のサイズは、入力の縦or横の次元数$N$、ウィンドウの縦or横の次元数$W$、ストライドの縦or横の量$S$で決まります。\n","    - $ceil((N-W+1)/S)$  (ceilは整数値に切り上げ)"]},{"cell_type":"markdown","metadata":{"id":"91reQTmj86Ly"},"source":["### プーリングの適用例 "]},{"cell_type":"code","metadata":{"id":"8e49ACyQ86Lz"},"source":["sample_image = np.array([[77, 80, 82, 78, 70],\n","                         [83, 78, 80, 83, 82],\n","                         [87, 82, 81, 80, 74],\n","                         [87, 87, 85, 77, 66],\n","                         [84, 79, 77, 78, 76]]\n","                         ).astype(\"float32\").reshape(1, 1, 5, 5) # バッチサイズ x 高さ x 幅 x チャンネル数s\n","\n","sample_image = torch.tensor(sample_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jpi31tv286L4"},"source":["#### プーリング1\n","\n","- ウィンドウサイズ: (2, 2)\n","- ストライド: (2, 2)\n","- プーリング: max\n","- ceil((5 -2+1) / 2) = 2"]},{"cell_type":"code","metadata":{"id":"sMsW1eJd86L6"},"source":["pooled_image = F.max_pool2d(sample_image, kernel_size=(2, 2), stride=(2, 2))\n","\n","print(pooled_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTrJiFvh86L-"},"source":["#### プーリング2\n","\n","- ウィンドウサイズ: (2, 2)\n","- ストライド: (1, 1)\n","- プーリング: max\n","- ceil((5 -2+1) / 1) = 4"]},{"cell_type":"code","metadata":{"id":"kpF_54hg86MB"},"source":["pooled_image = F.max_pool2d(sample_image, kernel_size=(2, 2), stride=(1, 1))\n","\n","print(pooled_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"khVKkNVw86MI"},"source":["#### プーリング3\n","\n","- ウィンドウサイズ: (2, 2)\n","- ストライド: (2, 2)\n","- プーリング: mean\n","- ceil((5 -2+1) / 2) = 2"]},{"cell_type":"code","metadata":{"id":"h6FGJOc-86MK"},"source":["pooled_image = F.avg_pool2d(sample_image, kernel_size=(2, 2), stride=(2, 2))\n","\n","print(pooled_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YC9PALgH86MV"},"source":["### プーリングの演習"]},{"cell_type":"code","metadata":{"id":"qb5SrwQx86MW"},"source":["sample_image = np.array([[77, 80, 82, 78, 70, 76, 75],\n","                         [83, 78, 78, 73, 82, 82, 85],\n","                         [87, 82, 81, 80, 74, 88, 70],\n","                         [87, 87, 85, 77, 66, 83, 87],\n","                         [81, 83, 77, 79, 66, 83, 87],\n","                         [87, 87, 83, 70, 66, 83, 87],\n","                         [84, 79, 77, 78, 76, 75, 80]]\n","                         ).astype(\"float32\").reshape(1, 1, 7, 7)\n","\n","sample_image = torch.tensor(sample_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a-S4R09386Mb"},"source":["#### プーリング1\n","\n","- ウィンドウサイズ: (2, 2)\n","- ストライド: (2, 2)\n","- プーリング: max"]},{"cell_type":"code","metadata":{"id":"EXJlCRKn86Mc"},"source":["pooled_image = F.max_pool2d(sample_image, kernel_size=(2, 2), stride=(2, 2))\n","\n","print(pooled_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTDoX5hV86Mg"},"source":["#### プーリング2\n","\n","- ウィンドウサイズ: (2, 2)\n","- ストライド: (1, 1)\n","- プーリング: max"]},{"cell_type":"code","metadata":{"id":"EBwsFU7686Mi"},"source":["pooled_image = F.max_pool2d(sample_image, kernel_size=(2, 2), stride=(1, 1))\n","\n","print(pooled_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ApMPRmr186Mm"},"source":["#### プーリング3\n","\n","- ウィンドウサイズ: (2, 2)\n","- ストライド: (2, 2)\n","- プーリング: mean"]},{"cell_type":"code","metadata":{"id":"59EexIhg86Mn"},"source":["pooled_image = F.avg_pool2d(sample_image, kernel_size=(2, 2), stride=(2, 2))\n","\n","print(pooled_image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qxMzp-Uu86Mr"},"source":["## 2. 各層クラスの実装"]},{"cell_type":"markdown","metadata":{"id":"xNgihkvm86Ms"},"source":["### 2.1.  畳み込み層"]},{"cell_type":"markdown","metadata":{"id":"TRxxpXPX86Mt"},"source":["活性化関数としてsigmoid関数やtanh関数のような対象な関数を使用する場合は、Xavierの初期化が使われることが多いです。以下の式で表されます。\n","\n","$\\displaystyle U(-\\sqrt{\\frac{6}{n_{\\mathrm{input}} + n_{\\mathrm{output}}}}, \\sqrt{\\frac{6}{n_{\\mathrm{input}} + n_{\\mathrm{output}}}})$\n","\n","$U$: 一様分布、 $ n_{input}$: 入力されるユニット数、$n_{output}$: 出力されるユニット数\n","\n","※ $ n_{input}$は1ニューロンが前層のいくつのニューロンとつながっているかを表します。\n","\n","今回の場合、非対称なReLUを活性化関数として使うので、Heの初期化を使用しています。以下の式で表されます。\n","\n","$\\displaystyle U(-\\sqrt{\\frac{6}{n_{\\mathrm{input}}}}, \\sqrt{\\frac{6}{n_{\\mathrm{input}}}})$\n","\n","$U$: 一様分布、 $ n_{input}$: 入力されるユニット数"]},{"cell_type":"code","metadata":{"id":"h-mhLGS-86Mu"},"source":["class Conv(nn.Module):\n","    def __init__(self, filter_shape, function=lambda x: x, stride=(1, 1), padding=0):\n","        super().__init__()\n","        # Heの初期値\n","        fan_in = filter_shape[1] * filter_shape[2] * filter_shape[3]\n","        # filter_shape: (出力チャンネル数)x(入力チャンネル数)x(縦の次元数)x(横の次元数)\n","        fan_out = filter_shape[0] * filter_shape[2] * filter_shape[3]\n","\n","        self.W = nn.Parameter(torch.tensor(rng.uniform(\n","                        -np.sqrt(6/fan_in),\n","                        np.sqrt(6/fan_in),\n","                        size=filter_shape\n","                    ).astype('float32')))\n","\n","        # バイアスはフィルタごとなので, 出力フィルタ数と同じ次元数\n","        self.b = nn.Parameter(torch.tensor(np.zeros((filter_shape[0]), dtype='float32')))\n","        self.function = function\n","        self.stride = stride\n","        self.padding = padding\n","        \n","    def forward(self, x):\n","        u = F.conv2d(x, self.W, bias=self.b, stride=self.stride, padding=self.padding)\n","        return self.function(u)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u0z5vVPH86Mx"},"source":["### 2.2. プーリング層"]},{"cell_type":"code","metadata":{"id":"06OjCDxr86Mx"},"source":["class Pooling(nn.Module):\n","    def __init__(self, ksize=(2, 2), stride=(2, 2), padding=0):\n","        super().__init__()\n","        self.ksize = ksize\n","        self.stride = stride\n","        self.padding = padding\n","\n","    def forward(self, x):\n","        return F.avg_pool2d(x, kernel_size=self.ksize, stride=self.stride, padding=self.padding)\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y56yq5ga86M3"},"source":["### 2.3. 平滑化層（4次元->2次元）"]},{"cell_type":"code","metadata":{"id":"zHiFHpN586M4"},"source":["class Flatten(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return x.view(x.size()[0], -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmsiZ-Zg86M-"},"source":["### 2.4. 全結合層"]},{"cell_type":"code","metadata":{"id":"omCauKXJ86NA"},"source":["class Dense(nn.Module):\n","    def __init__(self, in_dim, out_dim, function=lambda x: x):\n","        super().__init__()\n","        # He Initialization\n","        # in_dim: 入力の次元数、out_dim: 出力の次元数              \n","        self.W = nn.Parameter(torch.tensor(rng.uniform(\n","                        -np.sqrt(6/in_dim),\n","                        np.sqrt(6/in_dim),\n","                        size=(in_dim, out_dim)\n","                    ).astype('float32')))\n","\n","        self.b = nn.Parameter(torch.tensor(np.zeros([out_dim]).astype('float32')))\n","        self.function = function\n","    def forward(self, x):\n","        return self.function(torch.matmul(x, self.W) + self.b)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rzJQBUD86K4"},"source":["## 3. MNISTを用いたクラス分類"]},{"cell_type":"markdown","metadata":{"id":"k0cCMlbw86K6"},"source":["ここまでで定義したクラスを用いてMNISTのクラス分類を実装する。\n","\n","まずはモデルと、各種ハイパーパラメータ等を定義する\n"]},{"cell_type":"code","metadata":{"id":"lls6TFYCOMID"},"source":["conv_net = nn.Sequential(\n","    Conv((20, 1, 5, 5), F.relu),     # 28x28x 1 -> 24x24x20\n","    Pooling((2, 2)),                 # 24x24x20 -> 12x12x20\n","    Conv((50, 20, 5, 5), F.relu),    # 12x12x20 ->  8x 8x50\n","    Pooling((2, 2)),                 #  8x 8x50 ->  4x 4x50\n","    Flatten(),\n","    Dense(4*4*50, 10)\n",")\n","\n","batch_size = 100\n","n_epochs = 10\n","lr = 0.02\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","conv_net.to(device)\n","optimizer = optim.SGD(conv_net.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GORHrIND9dOk"},"source":["MNISTデータを読み込む"]},{"cell_type":"code","metadata":{"id":"oet7HSadGeIu"},"source":["# torchvisionのdatasetsを使ってMNISTのデータを取得\n","# ミニバッチ化や前処理などの処理を行ってくれるDataLoaderを定義\n","\n","dataloader_train = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/mnist', train=True, download=True, transform=transforms.ToTensor()),\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","dataloader_valid = torch.utils.data.DataLoader(\n","    datasets.MNIST('./data/mnist', train=False, download=True, transform=transforms.ToTensor()),\n","    batch_size=batch_size,\n","    shuffle=False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlTRf_Gj86NM"},"source":["## 4. 学習"]},{"cell_type":"code","metadata":{"id":"vZeEGdQE6pEJ"},"source":["for epoch in range(n_epochs):\n","    losses_train = []\n","    losses_valid = []\n","\n","    conv_net.train()\n","    n_train = 0\n","    acc_train = 0\n","    for x, t in dataloader_train:\n","        n_train += t.size()[0]\n","\n","        conv_net.zero_grad()  # 勾配の初期化\n","\n","        x = x.to(device)  # テンソルをGPUに移動\n","\n","        t_hot = torch.eye(10)[t]  # 正解ラベルをone-hot vector化\n","\n","        t_hot = t_hot.to(device)  # 正解ラベルとone-hot vectorをそれぞれGPUに移動\n","\n","        y = conv_net.forward(x)  # 順伝播\n","\n","        loss = -(t_hot*torch.log_softmax(y, dim=-1)).sum(axis=1).mean()  # 誤差(クロスエントロピー誤差関数)の計算\n","\n","        loss.backward()  # 誤差の逆伝播\n","\n","        optimizer.step()  # パラメータの更新\n","\n","        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n","\n","        acc_train += (pred.to(\"cpu\") == t).float().sum().item()\n","        losses_train.append(loss.tolist())\n","\n","    conv_net.eval()\n","    n_val = 0\n","    acc_val = 0\n","    for x, t in dataloader_valid:\n","        n_val += t.size()[0]\n","\n","        x = x.to(device)  # テンソルをGPUに移動\n","\n","        t_hot = torch.eye(10)[t]  # 正解ラベルをone-hot vector化\n","\n","        t_hot = t_hot.to(device)  # 正解ラベルとone-hot vectorをそれぞれGPUに移動\n","\n","        y = conv_net.forward(x)  # 順伝播\n","\n","        loss = -(t_hot*torch.log_softmax(y, dim=-1)).sum(axis=1).mean()  # 誤差(クロスエントロピー誤差関数)の計算\n","\n","        pred = y.argmax(1)  # 最大値を取るラベルを予測ラベルとする\n","\n","        acc_val += (pred.to(\"cpu\") == t).float().sum().item()\n","        losses_valid.append(loss.tolist())\n","\n","    print('EPOCH: {}, Train [Loss: {:.3f}, Accuracy: {:.3f}], Valid [Loss: {:.3f}, Accuracy: {:.3f}]'.format(\n","        epoch,\n","        np.mean(losses_train),\n","        acc_train/n_train,\n","        np.mean(losses_valid),\n","        acc_val/n_val\n","    ))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YF0WHXebvXH9"},"source":["\n"],"execution_count":null,"outputs":[]}]}